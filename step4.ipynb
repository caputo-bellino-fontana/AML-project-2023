{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New STEP4: MOVING TOWARDS FFreeDA - PRE-TRAINING PHASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO remove the following cell and add ToTensor to the transforms function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F\n",
    "import collections\n",
    "import numbers\n",
    "\n",
    "_pil_interpolation_to_str = {\n",
    "    Image.NEAREST: 'PIL.Image.NEAREST',\n",
    "    Image.BILINEAR: 'PIL.Image.BILINEAR',\n",
    "    Image.BICUBIC: 'PIL.Image.BICUBIC',\n",
    "    Image.LANCZOS: 'PIL.Image.LANCZOS',\n",
    "    Image.HAMMING: 'PIL.Image.HAMMING',\n",
    "    Image.BOX: 'PIL.Image.BOX',\n",
    "}\n",
    "\n",
    "class Resize(object):\n",
    "\n",
    "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
    "        assert isinstance(size, int) or (isinstance(size, collections.Iterable) and len(size) == 2)\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, img, lbl=None):\n",
    "        if lbl is not None:\n",
    "            return F.resize(img, self.size, self.interpolation), F.resize(lbl, self.size, Image.NEAREST)\n",
    "        else:\n",
    "            return F.resize(img, self.size, self.interpolation)\n",
    "\n",
    "    def __repr__(self):\n",
    "        interpolate_str = _pil_interpolation_to_str[self.interpolation]\n",
    "        return self.__class__.__name__ + '(size={0}, interpolation={1})'.format(self.size, interpolate_str)\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "\n",
    "    def __call__(self, pic, lbl=None):\n",
    "        if lbl is not None:\n",
    "            return F.to_tensor(pic), torch.from_numpy(np.array(lbl, dtype=np.uint8))\n",
    "        else:\n",
    "            return F.to_tensor(pic)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '()'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform  = Resize(size=(512, 1024))\n",
    "\n",
    "UNIFORM = True\n",
    "MAX_SAMPLE_PER_CLIENT = 20\n",
    "N_STYLE = 5\n",
    "TOT_CLIENT = 20\n",
    "FDA = True\n",
    "\n",
    "# Checkpoint and best model\n",
    "use_checkpoint = True \n",
    "CKPT_DIR = 'checkpoints'\n",
    "\n",
    "best_ckpt_path = f\"best_model_step4.tar\"\n",
    "ckpt_path = f\"model_step4.tar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GTA Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import VisionDataset\n",
    "\n",
    "class GTA5(VisionDataset):\n",
    "\n",
    "    labels2train = {\n",
    "        'cityscapes': {7: 0, 8: 1, 11: 2, 12: 3, 13: 4, 17: 5, 19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12,\n",
    "                       26: 13, 27: 14, 28: 15, 31: 16, 32: 17, 33: 18},\n",
    "    }\n",
    "\n",
    "    def __init__(self, root, transform=None, mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), cv2=False, target_dataset='cityscapes'):\n",
    "        assert target_dataset in GTA5.labels2train, f'Class mapping missing for {target_dataset}, choose from: {GTA5.labels2train.keys()}'\n",
    "        self.labels2train = GTA5.labels2train[target_dataset]\n",
    "\n",
    "        #super().__init__(root, transform=transform, target_transform=None)\n",
    "\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.cv2 = cv2\n",
    "\n",
    "        self.target_transform = self.__map_labels()\n",
    "\n",
    "        self.return_unprocessed_image = False\n",
    "        self.style_tf_fn = None\n",
    "\n",
    "        with open(os.path.join(self.root,'train.txt'), \"r\") as f:\n",
    "          lines = f.readlines()\n",
    "\n",
    "        # manipulate each file row in order to obtain the correct path \n",
    "        self.paths_images = [l.strip() for l in lines]\n",
    "        # self.paths_tagets = [l for l in lines]\n",
    "\n",
    "        self.len = len(self.paths_images)\n",
    "\n",
    "    def set_style_tf_fn(self, style_tf_fn):\n",
    "        self.style_tf_fn = style_tf_fn\n",
    "\n",
    "    def reset_style_tf_fn(self):\n",
    "        self.style_tf_fn = None\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x_path = os.path.join(self.root,'images',self.paths_images[index])\n",
    "        y_path = os.path.join(self.root,'labels',self.paths_images[index])\n",
    " \n",
    "        x = Image.open(x_path)\n",
    "        y = Image.open(y_path) \n",
    "\n",
    "        ## using read_image\n",
    "        # x = read_image(x_path)\n",
    "        # y = read_image(y_path)\n",
    "\n",
    "        if self.return_unprocessed_image:\n",
    "            return x\n",
    "        if self.style_tf_fn is not None:\n",
    "            x = self.style_tf_fn(x)\n",
    "        if self.transform is not None:\n",
    "          x, y = self.transform(x, y)\n",
    "        y = self.target_transform(y)\n",
    "\n",
    "# TODO: insert directly in the transform Compose ?? \n",
    "        transform_Tensor = ToTensor()\n",
    "        x, y = transform_Tensor(x, y)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __map_labels(self):\n",
    "        mapping = np.zeros((256,), dtype=np.int64) + 255\n",
    "        for k, v in self.labels2train.items():\n",
    "            mapping[k] = v\n",
    "        return lambda x: from_numpy(mapping[x])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cityscapes Client Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch import from_numpy\n",
    "from torch.utils import data\n",
    "      \n",
    "class CityscapesClient(data.Dataset):\n",
    "\n",
    "    labels2train = {7: 0, 8: 1, 11: 2, 12: 3, 13: 4, 17: 5, 19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12, 26: 13,\n",
    "                    27: 14, 28: 15, 31: 16, 32: 17, 33: 18}\n",
    "\n",
    "    def __init__(self, root, uniform, id_client, transform=None, mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)):\n",
    "\n",
    "        self.root = root\n",
    "\n",
    "        if uniform:\n",
    "          filename = 'uniformA.json'\n",
    "        else:\n",
    "          filename = 'heterogeneuosA.json'\n",
    "\n",
    "        with open(os.path.join(root,filename)) as f:\n",
    "          dict_data = json.load(f)\n",
    "\n",
    "        self.paths_images = [l[0] for l in dict_data[str(id_client)]]\n",
    "        self.paths_tagets = [l[1] for l in dict_data[str(id_client)]]\n",
    "\n",
    "        self.len = len(self.paths_images)\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.mean = mean\n",
    "        self.std = std \n",
    "        self.test = False\n",
    "        self.style_tf_fn = None\n",
    "        self.return_unprocessed_image = False\n",
    "\n",
    "        self.target_transform = self.__map_labels()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = Image.open(os.path.join(self.root,\"images\",self.paths_images[index]))\n",
    "        y = Image.open(os.path.join(self.root,\"labels\",self.paths_tagets[index])) \n",
    "        \n",
    "        if self.return_unprocessed_image:\n",
    "            return x\n",
    "            \n",
    " \n",
    "        if self.transform is not None:\n",
    "          x, y = self.transform(x, y)\n",
    "          \n",
    "        y = self.target_transform(y)\n",
    "\n",
    "# TODO: insert directly in the transform Compose ?? \n",
    "        transform_Tensor = ToTensor()\n",
    "        x, y = transform_Tensor(x, y)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __map_labels(self):\n",
    "        mapping = np.zeros((256,), dtype=np.int64) + 255\n",
    "        for k, v in self.labels2train.items():\n",
    "            mapping[k] = v\n",
    "        return lambda x: from_numpy(mapping[x])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style Augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class StyleAugment:\n",
    "\n",
    "    def __init__(self, n_images_per_style=10, L=0.1, size=(1024, 512), b=None):\n",
    "        self.styles = []\n",
    "        self.styles_names = []\n",
    "        self.n_images_per_style = n_images_per_style\n",
    "        self.L = L\n",
    "        self.size = size\n",
    "        self.sizes = None\n",
    "        self.cv2 = False\n",
    "        self.b = b\n",
    "\n",
    "    def preprocess(self, x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = cv2.resize(x, self.size, interpolation=cv2.INTER_CUBIC)\n",
    "            self.cv2 = True\n",
    "        else:\n",
    "            x = x.resize(self.size, Image.BICUBIC)\n",
    "        x = np.asarray(x, np.float32)\n",
    "        x = x[:, :, ::-1]\n",
    "        x = x.transpose((2, 0, 1))\n",
    "        return x.copy()\n",
    "\n",
    "    def deprocess(self, x, size):\n",
    "        if self.cv2:\n",
    "            x = cv2.resize(np.uint8(x).transpose((1, 2, 0))[:, :, ::-1], size, interpolation=cv2.INTER_CUBIC)\n",
    "        else:\n",
    "            x = Image.fromarray(np.uint8(x).transpose((1, 2, 0))[:, :, ::-1])\n",
    "            x = x.resize(size, Image.BICUBIC)\n",
    "        return x\n",
    "\n",
    "    def add_style(self, loader, multiple_styles=False, name=None):\n",
    "        if self.n_images_per_style < 0:\n",
    "            return\n",
    "\n",
    "        if name is not None:\n",
    "            self.styles_names.append([name] * self.n_images_per_style if multiple_styles else [name])\n",
    "\n",
    "        loader.return_unprocessed_image = True\n",
    "        n = 0\n",
    "        styles = []\n",
    "\n",
    "        for sample in tqdm(loader, total=min(len(loader), self.n_images_per_style)):\n",
    "\n",
    "            image = self.preprocess(sample)\n",
    "\n",
    "            if n >= self.n_images_per_style:\n",
    "                break\n",
    "            styles.append(self._extract_style(image))\n",
    "            n += 1\n",
    "\n",
    "        if self.n_images_per_style > 1:\n",
    "            if multiple_styles:\n",
    "                self.styles += styles\n",
    "            else:\n",
    "                styles = np.stack(styles, axis=0)\n",
    "                style = np.mean(styles, axis=0)\n",
    "                self.styles.append(style)\n",
    "        elif self.n_images_per_style == 1:\n",
    "            self.styles += styles\n",
    "\n",
    "        loader.return_unprocessed_image = False\n",
    "\n",
    "    def _extract_style(self, img_np):\n",
    "        fft_np = np.fft.fft2(img_np, axes=(-2, -1))\n",
    "        amp = np.abs(fft_np)\n",
    "        amp_shift = np.fft.fftshift(amp, axes=(-2, -1))\n",
    "        if self.sizes is None:\n",
    "            self.sizes = self.compute_size(amp_shift)\n",
    "        h1, h2, w1, w2 = self.sizes\n",
    "        style = amp_shift[:, h1:h2, w1:w2]\n",
    "        return style\n",
    "\n",
    "    def compute_size(self, amp_shift):\n",
    "        _, h, w = amp_shift.shape\n",
    "        b = (np.floor(np.amin((h, w)) * self.L)).astype(int) if self.b is None else self.b\n",
    "        c_h = np.floor(h / 2.0).astype(int)\n",
    "        c_w = np.floor(w / 2.0).astype(int)\n",
    "        h1 = c_h - b\n",
    "        h2 = c_h + b + 1\n",
    "        w1 = c_w - b\n",
    "        w2 = c_w + b + 1\n",
    "        return h1, h2, w1, w2\n",
    "\n",
    "    def apply_style(self, image):\n",
    "        return self._apply_style(image)\n",
    "\n",
    "    def _apply_style(self, img):\n",
    "\n",
    "        if self.n_images_per_style < 0:\n",
    "            return img\n",
    "\n",
    "        if len(self.styles) > 0:\n",
    "            n = random.randint(0, len(self.styles) - 1)\n",
    "            style = self.styles[n]\n",
    "        else:\n",
    "            style = self.styles[0]\n",
    "\n",
    "        if isinstance(img, np.ndarray):\n",
    "            H, W = img.shape[0:2]\n",
    "        else:\n",
    "            W, H = img.size\n",
    "        img_np = self.preprocess(img)\n",
    "\n",
    "        fft_np = np.fft.fft2(img_np, axes=(-2, -1))\n",
    "        amp, pha = np.abs(fft_np), np.angle(fft_np)\n",
    "        amp_shift = np.fft.fftshift(amp, axes=(-2, -1))\n",
    "        h1, h2, w1, w2 = self.sizes\n",
    "        amp_shift[:, h1:h2, w1:w2] = style\n",
    "        amp_ = np.fft.ifftshift(amp_shift, axes=(-2, -1))\n",
    "\n",
    "        fft_ = amp_ * np.exp(1j * pha)\n",
    "        img_np_ = np.fft.ifft2(fft_, axes=(-2, -1))\n",
    "        img_np_ = np.real(img_np_)\n",
    "        img_np__ = np.clip(np.round(img_np_), 0., 255.)\n",
    "\n",
    "        img_with_style = self.deprocess(img_np__, (W, H))\n",
    "\n",
    "        return img_with_style\n",
    "\n",
    "    def test(self, images_np, images_target_np=None, size=None):\n",
    "\n",
    "        Image.fromarray(np.uint8(images_np.transpose((1, 2, 0)))[:, :, ::-1]).show()\n",
    "        fft_np = np.fft.fft2(images_np, axes=(-2, -1))\n",
    "        amp = np.abs(fft_np)\n",
    "        amp_shift = np.fft.fftshift(amp, axes=(-2, -1))\n",
    "        h1, h2, w1, w2 = self.sizes\n",
    "        style = amp_shift[:, h1:h2, w1:w2]\n",
    "\n",
    "        fft_np_ = np.fft.fft2(images_np if images_target_np is None else images_target_np, axes=(-2, -1))\n",
    "        amp_, pha_ = np.abs(fft_np_), np.angle(fft_np_)\n",
    "        amp_shift_ = np.fft.fftshift(amp_, axes=(-2, -1))\n",
    "        h1, h2, w1, w2 = self.sizes\n",
    "        amp_shift_[:, h1:h2, w1:w2] = style\n",
    "        amp__ = np.fft.ifftshift(amp_shift_, axes=(-2, -1))\n",
    "\n",
    "        fft_ = amp__ * np.exp(1j * pha_)\n",
    "        img_np_ = np.fft.ifft2(fft_, axes=(-2, -1))\n",
    "        img_np_ = np.real(img_np_)\n",
    "        img_np__ = np.clip(np.round(img_np_), 0., 255.)\n",
    "        Image.fromarray(np.uint8(images_target_np.transpose((1, 2, 0)))[:, :, ::-1]).show()\n",
    "        Image.fromarray(np.uint8(img_np__).transpose((1, 2, 0))[:, :, ::-1]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GTA dataset visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Resize(size=(512, 1024))\n",
    "train_dataset = GTA5(root=ROOT_DIR_GTA5, transform=transform)\n",
    "print(\"Dataset dimension: \", len(train_dataset))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "\n",
    "rows = 1\n",
    "columns = 2\n",
    "for i, (imgs, targets) in enumerate(train_dataloader):\n",
    "    print(imgs[0].shape,targets[0].shape)\n",
    "\n",
    "    figure = plt.figure(figsize=(10,20))\n",
    "    figure.add_subplot(rows, columns,1)\n",
    "    print(\"img type:\",type(imgs[0]), \" target:\",type(targets[0]))\n",
    "    print(\"img:\", imgs[0].squeeze().shape, \" target:\",targets[0].squeeze().shape)\n",
    "  \n",
    "\n",
    "    plt.imshow(imgs[0].permute((1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Image\")\n",
    "    figure.add_subplot(rows, columns,2)\n",
    "    \n",
    "    plt.imshow(decode_segmap(targets[0]))\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Groundtruth\")\n",
    "    plt.show()\n",
    "    \n",
    "    if i == 1: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FDA:\n",
    "  #L 0.01, 0.05, 0.09\n",
    "  # b == 0 --> 1x1, b == 1 --> 3x3, b == 2 --> 5x5, ...'\n",
    "  SA = StyleAugment(n_images_per_style=MAX_SAMPLE_PER_CLIENT, L=0.01, size=(1024, 512), b=1) \n",
    "\n",
    "  clients = random.sample([_ for _ in range(TOT_CLIENT)],N_STYLE)\n",
    "  for c in clients:\n",
    "    client_dataset = CityscapesClient(root=ROOT_DIR, uniform=UNIFORM, id_client=c, transform=transform)\n",
    "    SA.add_style(client_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimension:  500\n"
     ]
    }
   ],
   "source": [
    "train_dataset = GTA5(root=ROOT_DIR_GTA5, transform=transform)\n",
    "\n",
    "if FDA:\n",
    "  train_dataset.set_style_tf_fn(SA.apply_style)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "print(\"Dataset dimension: \", len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GTA images with Cityscapes style visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 2\n",
    "rows = 1\n",
    "columns = 2\n",
    "for i, (imgs, targets) in enumerate(train_dataloader):\n",
    "    print(imgs[0].shape,targets[0].shape)\n",
    "\n",
    "    figure = plt.figure(figsize=(50,50))\n",
    "    figure.add_subplot(rows, columns,1)\n",
    "    print(\"img type:\",type(imgs[0]), \" target:\",type(targets[0]))\n",
    "    print(\"img:\", imgs[0].squeeze().shape, \" target:\",targets[0].squeeze().shape)\n",
    "\n",
    "    plt.imshow(imgs[0].permute((1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Image\")\n",
    "    figure.add_subplot(rows, columns,2)\n",
    "    \n",
    "    plt.imshow(decode_segmap(targets[0]))\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Groundtruth\")\n",
    "    #plt.show()\n",
    "    plt.savefig(f\"style_{i}.png\", transparent = True)\n",
    "    if i+1 == n_images: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform  = Resize(size=(512, 1024))\n",
    "client_dataset = CityscapesClient(root=ROOT_DIR, uniform=True, id_client=0, transform=transform)\n",
    "val_dataloader = DataLoader(client_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(CKPT_DIR):\n",
    "  os.mkdir(CKPT_DIR)\n",
    "\n",
    "if use_checkpoint and ckpt_path in os.listdir(CKPT_DIR):\n",
    "  checkpoint = torch.load(os.path.join(CKPT_DIR,ckpt_path))\n",
    "  print('Epoch {}, Loss {}, MIoU {}'.format(checkpoint['epoch'], checkpoint['loss'], checkpoint['miou']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiSeNetV2(NUM_CLASSES, output_aux=False, pretrained=True)\n",
    "if use_checkpoint and ckpt_path in os.listdir(CKPT_DIR):\n",
    "  model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index= 255) \n",
    "\n",
    "parameters_to_optimize = model.parameters()\n",
    "optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "if use_checkpoint and ckpt_path in os.listdir(CKPT_DIR):\n",
    "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark # Calling this optimizes runtime\n",
    "\n",
    "losses = []\n",
    "losses_val = []\n",
    "epochs = []\n",
    "\n",
    "# wandb.watch(model, log='all')\n",
    "\n",
    "if best_ckpt_path in os.listdir(CKPT_DIR):\n",
    "    best_checkpoint = torch.load(os.path.join(CKPT_DIR,best_ckpt_path))\n",
    "    best_loss = best_checkpoint['loss']\n",
    "    best_miou = best_checkpoint['miou']\n",
    "\n",
    "\n",
    "current_step = 0\n",
    "start_epoch = 0\n",
    "if use_checkpoint and ckpt_path in os.listdir(CKPT_DIR):\n",
    "  start_epoch = checkpoint['epoch']\n",
    "\n",
    "# Start iterating over the epochs\n",
    "for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "  print('Starting epoch {}/{}'.format(epoch+1, NUM_EPOCHS))\n",
    "  epochs.append(epoch+1)\n",
    "\n",
    "  # Iterate over the dataset\n",
    "  for images, labels in train_dataloader:\n",
    "\n",
    "    images = images.to(DEVICE, dtype=torch.float32)\n",
    "    labels = labels.to(DEVICE, dtype=torch.long)\n",
    "    #print(\"images:\",images.shape,\"labels:\",labels.shape)\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    predictions = model(images)\n",
    "    #print(\"predictions:\",predictions.shape,\"labels:\",labels.shape)\n",
    "    loss = criterion(predictions, labels.squeeze())\n",
    "\n",
    "    #wandb.log({\"train/loss\":loss})\n",
    "\n",
    "    # Log loss\n",
    "    if current_step % LOG_FREQUENCY == 0:\n",
    "      miou = compute_moiu(model, val_dataloader)\n",
    "      print('Step {}, Loss {}, MIoU {}'.format(current_step, loss.item(),miou))\n",
    "      # wandb.log({\"train/loss\": loss})\n",
    "    # Compute gradients for each layer and update weights\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    current_step += 1\n",
    "\n",
    "  # save intermediate checkpoint\n",
    "  torch.save({'epoch': epoch+1, 'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(), 'loss': loss, 'miou': miou}, \n",
    "             os.path.join(CKPT_DIR,ckpt_path))\n",
    "  \n",
    "  # save/update best checkpoint\n",
    "  if best_ckpt_path not in os.listdir(CKPT_DIR) or (best_ckpt_path in os.listdir(CKPT_DIR) and best_miou < miou):\n",
    "    torch.save({'epoch': epoch+1, 'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(), 'loss': loss, 'miou': miou}, \n",
    "               os.path.join(CKPT_DIR,best_ckpt_path))\n",
    "    best_loss = loss\n",
    "    best_miou = miou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if best_ckpt_path in os.listdir(CKPT_DIR):\n",
    "  best_checkpoint = torch.load(os.path.join(CKPT_DIR,best_ckpt_path))\n",
    "  model = BiSeNetV2(NUM_CLASSES, output_aux=False, pretrained=True)\n",
    "  model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "  # Set dropout and batch normalization layers to evaluation mode before running inference.\n",
    "  model.eval() \n",
    "  print()\n",
    "else: \n",
    "  print('There is no model to load')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimension:  19\n"
     ]
    }
   ],
   "source": [
    "transform = Resize(size=(512, 1024))\n",
    "val_dataset = CityscapesClient(root=ROOT_DIR, uniform=True, id_client=10, transform=transform)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "print(\"Dataset dimension: \", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_plot(net= model, val_dataloader=val_dataloader, n_image=20)\n",
    "torch.cuda.empty_cache()\n",
    "miou = compute_moiu(net=model, val_dataloader=val_dataloader)\n",
    "print('Validation MIoU: {}'.format(miou))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
