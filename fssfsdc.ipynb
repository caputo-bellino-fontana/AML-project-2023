{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnLNqTus6-9e"
      },
      "source": [
        "# Federated Semantic Segmentation for Self Driving Cars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCoROSvT6-9f"
      },
      "source": [
        "## STEP 0: SETUP PHASE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnQPPHnH6-9f"
      },
      "source": [
        "### Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iO5GFa6a6-9f",
        "outputId": "edc60bfb-155a-46ce-e104-d5b1de137f43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 KB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (2.0.0+cu118)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.10.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.11.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.14.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.20.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.19.1-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.27.1)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (67.6.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=e1e504ce7d0682b2cdbd5734ff0a1572b06c37a2be353f6b51f431b74746c15c\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.19.1 setproctitle-1.3.2 smmap-5.0.0 wandb-0.14.2\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_32w_Ap6-9g"
      },
      "source": [
        "### Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfqCMdIx6-9g",
        "outputId": "c15926db-0547-46e0-b096-93b7cfcf2604"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMea9CDF6-9g",
        "outputId": "aea202c5-5eaa-479c-b9ba-ee1d9902d7a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/AMLproject\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/AMLproject/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVGBAN-H6-9g"
      },
      "source": [
        "### Cloning FedDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "54lEAIhO6-9g"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if not os.path.isdir('./FedDrive'):\n",
        "  !git clone https://github.com/Erosinho13/FedDrive\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u84oVILW6-9g"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hvT1tbH76-9h"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as torch_data\n",
        "import torch.nn.functional as F \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "import random\n",
        "# import torchvision\n",
        "# import logging\n",
        "import warnings\n",
        "import math\n",
        "import json\n",
        "import wandb\n",
        "# TODO: remove all cv2 code\n",
        "import cv2\n",
        "\n",
        "# from torchvision import transforms\n",
        "from torch.backends import cudnn\n",
        "from torch import from_numpy\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader\n",
        "from torchmetrics.classification import MulticlassJaccardIndex\n",
        "from torchvision.datasets import VisionDataset\n",
        "from collections import OrderedDict\n",
        "\n",
        "from drive.MyDrive.AMLproject import transform as T\n",
        "from FedDrive.src.modules.bisenetv2 import BiSeNetV2\n",
        "\n",
        "warnings.resetwarnings()\n",
        "warnings.simplefilter('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wf3LLgTc6-9h"
      },
      "source": [
        "### Parameter Configuration Step 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jn6-QYWG6-9h"
      },
      "outputs": [],
      "source": [
        "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "PARTITION = 'A' # A or B to choose which dataloader to use\n",
        "SEED = 42\n",
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "\n",
        "NUM_CLASSES = 19\n",
        "if NUM_CLASSES == 19:\n",
        "  cl19 = True\n",
        "\n",
        "BATCH_SIZE = 8     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing ***\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = 0.05           # The initial Learning Rate ***\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD ***\n",
        "WEIGHT_DECAY =0.0005  # Regularization, you can keep this at the default ***\n",
        "\n",
        "NUM_EPOCHS = 10     # Total number of training epochs (iterations over dataset)\n",
        "\n",
        "# servono per decrementare il lerning rate nel tempo \n",
        "# STEP_SIZE = 500       # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "# GAMMA = 0.8          # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = 20\n",
        "LOG_FREQUENCY_EPOCH = 3\n",
        "\n",
        "ROOT_DIR = os.path.join('data', 'Cityscapes')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0IawvAn6-9h"
      },
      "source": [
        "### Parameter configuration Step 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LumnY7ti6-9h"
      },
      "outputs": [],
      "source": [
        "PARTITION = \"B\"  # 'A' or 'B'\n",
        "SPLIT = 1  # 1 or 2 // 1 = Uniform : 2 = Heterogenous\n",
        "MAX_SAMPLE_PER_CLIENT = 20\n",
        "\n",
        "IMAGES_FINAL = \"leftImg8bit\"\n",
        "TARGET_FINAL = \"gtFine_labelIds\"\n",
        "\n",
        "N_ROUND = 50\n",
        "CLIENT_PER_ROUND = 5  # clients picked each round\n",
        "NUM_EPOCHS = 2\n",
        "\n",
        "\n",
        "CHECKPOINTS = 5\n",
        "\n",
        "\n",
        "if PARTITION == 'A':\n",
        "  if SPLIT == 1:\n",
        "    TOT_CLIENTS = 36\n",
        "  else:\n",
        "    TOT_CLIENTS = 46\n",
        "else:\n",
        "  if SPLIT == 1:\n",
        "    TOT_CLIENTS = 25\n",
        "  else:\n",
        "    TOT_CLIENTS = 33\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKzXeSPSpHKe"
      },
      "source": [
        "### Parameter configuration Step 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWuGuUnUpHKe"
      },
      "outputs": [],
      "source": [
        "PARTITION = 'A' # A or B to choose which dataloader to use\n",
        "SPLIT = 1  # 1 or 2 // 1 = Uniform : 2 = Heterogenous\n",
        "\n",
        "SEED = 42\n",
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "\n",
        "NUM_CLASSES = 19\n",
        "cl19 = True\n",
        "\n",
        "BATCH_SIZE = 8     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing ***\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = 0.05           # The initial Learning Rate ***\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD ***\n",
        "WEIGHT_DECAY = 0.0005  # Regularization, you can keep this at the default ***\n",
        "\n",
        "NUM_EPOCHS = 10     # Total number of training epochs (iterations over dataset)\n",
        "\n",
        "# servono per decrementare il lerning rate nel tempo \n",
        "# STEP_SIZE = 500       # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "# GAMMA = 0.8          # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = 20\n",
        "LOG_FREQUENCY_EPOCH = 3\n",
        "\n",
        "\n",
        "LOAD_CKPT_PATH = 'ckpt_0.pth'\n",
        "CKPT_PATH = './checkpoints/'\n",
        "CKPT_DIR = './checkpoints/'\n",
        "LOAD_CKPT = False\n",
        "\n",
        "# TODO: move\n",
        "if not os.path.isdir(CKPT_DIR):\n",
        "  os.mkdir(CKPT_DIR)\n",
        "\n",
        "CTSC_ROOT = \"./data/Cityscapes/\"\n",
        "GTA5_ROOT = \"./data/GTA5/\"\n",
        "\n",
        "# For FDA \n",
        "FDA = True\n",
        "MAX_SAMPLE_PER_CLIENT = 20\n",
        "N_STYLE = 5\n",
        "# b == 0 --> 1x1, b == 1 --> 3x3, b == 2 --> 5x5, ...'\n",
        "BETA_WINDOW_SIZE = 1\n",
        "\n",
        "if PARTITION == 'A':\n",
        "  if SPLIT == 1:\n",
        "    TOT_CLIENTS = 36\n",
        "  else:\n",
        "    TOT_CLIENTS = 46\n",
        "else:\n",
        "  if SPLIT == 1:\n",
        "    TOT_CLIENTS = 25\n",
        "  else:\n",
        "    TOT_CLIENTS = 33\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Vla14vbEQMt"
      },
      "source": [
        "### Parameter configuration Step 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "10RcwmewtQYX"
      },
      "outputs": [],
      "source": [
        "T_ROUND = 2 \n",
        "PSEUDO_LAB = True\n",
        "FDA = False\n",
        "\n",
        "PARTITION = \"A\"  # 'A' or 'B'\n",
        "SPLIT = 1  # 1 or 2 // 1 = Uniform : 2 = Heterogenous\n",
        "\n",
        "SEED = 42\n",
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "\n",
        "NUM_CLASSES = 19\n",
        "if NUM_CLASSES == 19:\n",
        "  cl19 = True  \n",
        "\n",
        "BATCH_SIZE = 8     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing ***\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = 0.05           # The initial Learning Rate ***\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD ***\n",
        "WEIGHT_DECAY = 0.0005  # Regularization, you can keep this at the default ***\n",
        "\n",
        "LOG_FREQUENCY = 20\n",
        "LOG_FREQUENCY_EPOCH = 3\n",
        "\n",
        "MAX_SAMPLE_PER_CLIENT = 20\n",
        "\n",
        "IMAGES_FINAL = \"leftImg8bit\"\n",
        "TARGET_FINAL = \"gtFine_labelIds\"\n",
        "\n",
        "N_ROUND = 50\n",
        "CLIENT_PER_ROUND = 5  # clients picked each round\n",
        "NUM_EPOCHS = 2\n",
        "\n",
        "CHECKPOINTS = 5\n",
        "\n",
        "CTSC_ROOT = \"./data/Cityscapes/\"\n",
        "GTA5_ROOT = \"./data/GTA5/\"\n",
        "\n",
        "# Used to load the model from the previous step\n",
        "ckpt_path = 'step4_A_pretraining_0.15.pth'\n",
        "CKPT_DIR = './checkpoints'\n",
        "\n",
        "if PARTITION == 'A':\n",
        "  if SPLIT == 1:\n",
        "    TOT_CLIENTS = 36\n",
        "  else:\n",
        "    TOT_CLIENTS = 46\n",
        "else:\n",
        "  if SPLIT == 1:\n",
        "    TOT_CLIENTS = 25\n",
        "  else:\n",
        "    TOT_CLIENTS = 33\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIorjeE46-9i"
      },
      "source": [
        "### Data Augmentation Options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Q8m3wblz6-9i"
      },
      "outputs": [],
      "source": [
        "# data augmentation options\n",
        "# set as None if a transformation should not be used\n",
        "\n",
        "RANDOM_HORIZONTAL_FLIP = 0.5\n",
        "#RANDOM_HORIZONTAL_FLIP = None      #probability of the image being flipped\n",
        "#COLOR_JITTER = (0.2,0.3,0.2,0.2) # (brighteness, contrast, saturation, hue)\n",
        "COLOR_JITTER = None\n",
        "RANDOM_ROTATION = 5              # degree of rotation\n",
        "#RANDOM_ROTATION = None\n",
        "#RANDOM_CROP = (512,1024)         # output size of the crop\n",
        "RANDOM_CROP = None\n",
        "RESIZE = (512,1024)              # output size\n",
        "#RESIZE = None\n",
        "#RANDOM_VERTICAL_FLIP  = 0.3     # probability of the image being flipped\n",
        "RANDOM_VERTICAL_FLIP = None\n",
        "CENTRAL_CROP = (512,1024)\n",
        "#CENTRAL_CROP = (512,1024)\n",
        "#RANDOM_RESIZE_CROP = (1024,2048)\n",
        "RANDOM_RESIZE_CROP = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsPNj7yC6-9i"
      },
      "source": [
        "### Transforms setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "t5iT7E3x6-9i"
      },
      "outputs": [],
      "source": [
        "def setup_transform():\n",
        "    transformers = []\n",
        "    if RANDOM_HORIZONTAL_FLIP is not None:\n",
        "        transformers.append(T.RandomHorizontalFlip(RANDOM_HORIZONTAL_FLIP))\n",
        "    if COLOR_JITTER is not None:\n",
        "        transformers.append(T.ColorJitter(*COLOR_JITTER))\n",
        "    if RANDOM_ROTATION is not None:\n",
        "        transformers.append(T.RandomRotation(RANDOM_ROTATION))\n",
        "    if RANDOM_CROP is not None:\n",
        "        transformers.append(T.RandomCrop(RANDOM_CROP))\n",
        "    if RANDOM_VERTICAL_FLIP is not None:\n",
        "        transformers.append(T.RandomVerticalFlip(RANDOM_VERTICAL_FLIP))\n",
        "    if CENTRAL_CROP is not None:\n",
        "        transformers.append(T.CenterCrop(CENTRAL_CROP))\n",
        "    if RANDOM_RESIZE_CROP is not None:\n",
        "        transformers.append(T.RandomResizedCrop(RANDOM_RESIZE_CROP))\n",
        "    if RESIZE is not None:\n",
        "        transformers.append(T.Resize(RESIZE))\n",
        "\n",
        "    transformers.append(T.ToTensor())\n",
        "\n",
        "    transforms = T.Compose(transformers)\n",
        "\n",
        "    return transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVFRMBjZ1t-S"
      },
      "source": [
        "### Fixing random seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NSCJKIVn1wDh"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1SkAQlk6-9i"
      },
      "source": [
        "### Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOrGP70fvC9r"
      },
      "source": [
        "#### Cityscapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "A34rFp_p6-9i"
      },
      "outputs": [],
      "source": [
        "class Cityscapes(torch_data.Dataset):\n",
        "\n",
        "    \"\"\"\n",
        "    image path: data/Cityscapes/images/name_leftImg8bit.png\n",
        "    taget path: data/Cityscapes/labels/name_gtFine_labelIds.png\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root, transform=None, cl19=False, filename=None, id_client=None):\n",
        "        eval_classes = [\n",
        "            7,\n",
        "            8,\n",
        "            11,\n",
        "            12,\n",
        "            13,\n",
        "            17,\n",
        "            19,\n",
        "            20,\n",
        "            21,\n",
        "            22,\n",
        "            23,\n",
        "            24,\n",
        "            25,\n",
        "            26,\n",
        "            27,\n",
        "            28,\n",
        "            31,\n",
        "            32,\n",
        "            33,\n",
        "        ]\n",
        "        self.root = root\n",
        "\n",
        "        if filename is None:\n",
        "            raise ValueError(\"filename is None\")\n",
        "\n",
        "        if id_client is not None:\n",
        "            with open(os.path.join(root, filename)) as f:\n",
        "                dict_data = json.load(f)\n",
        "\n",
        "            self.paths_images = [l[0] for l in dict_data[str(id_client)]]\n",
        "            self.paths_tagets = [l[1] for l in dict_data[str(id_client)]]\n",
        "\n",
        "        else:\n",
        "            with open(os.path.join(root, filename), \"r\") as f:\n",
        "                lines = f.readlines()\n",
        "\n",
        "            # manipulate each file row in order to obtain the correct path\n",
        "            self.paths_images = [l.strip().split(\"@\")[0] for l in lines]\n",
        "            self.paths_tagets = [l.strip().split(\"@\")[1] for l in lines]\n",
        "\n",
        "            # self.len = len(self.paths_images)\n",
        "            # self.transform = transform\n",
        "\n",
        "        self.len = len(self.paths_images)\n",
        "        self.transform = transform\n",
        "        self.return_unprocessed_image = False\n",
        "\n",
        "        if cl19:\n",
        "            classes = eval_classes\n",
        "            mapping = np.zeros((256,), dtype=np.int64) + 255\n",
        "            for i, cl in enumerate(classes):\n",
        "                mapping[cl] = i\n",
        "            self.target_transform = lambda x: from_numpy(mapping[x])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is the label of segmentation.\n",
        "        \"\"\"\n",
        "\n",
        "        # # using PIL\n",
        "        img = Image.open(os.path.join(self.root,\"images\",self.paths_images[index]))\n",
        "        target = Image.open(os.path.join(self.root,\"labels\",self.paths_tagets[index]))\n",
        "\n",
        "        if self.return_unprocessed_image:\n",
        "            return img\n",
        "\n",
        "        if self.transform:\n",
        "            img, target = self.transform(img, target)\n",
        "\n",
        "        if self.target_transform:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target  # output: Tensor[image_channels, image_height, image_width]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKcn7wbRvFo2"
      },
      "source": [
        "#### GTA5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YVSlXZcMpHKg"
      },
      "outputs": [],
      "source": [
        "class GTA5(VisionDataset):\n",
        "    labels2train = {\n",
        "        \"cityscapes\": {\n",
        "            7: 0,\n",
        "            8: 1,\n",
        "            11: 2,\n",
        "            12: 3,\n",
        "            13: 4,\n",
        "            17: 5,\n",
        "            19: 6,\n",
        "            20: 7,\n",
        "            21: 8,\n",
        "            22: 9,\n",
        "            23: 10,\n",
        "            24: 11,\n",
        "            25: 12,\n",
        "            26: 13,\n",
        "            27: 14,\n",
        "            28: 15,\n",
        "            31: 16,\n",
        "            32: 17,\n",
        "            33: 18,\n",
        "        },\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        root,\n",
        "        transform=None,\n",
        "        mean=(0.5, 0.5, 0.5),\n",
        "        std=(0.5, 0.5, 0.5),\n",
        "        cv2=False,\n",
        "        target_dataset=\"cityscapes\",\n",
        "    ):\n",
        "        assert (\n",
        "            target_dataset in GTA5.labels2train\n",
        "        ), f\"Class mapping missing for {target_dataset}, choose from: {GTA5.labels2train.keys()}\"\n",
        "\n",
        "        self.labels2train = GTA5.labels2train[target_dataset]\n",
        "\n",
        "        # super().__init__(root, transform=transform, target_transform=None)\n",
        "\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.cv2 = cv2\n",
        "\n",
        "        self.target_transform = self.__map_labels()\n",
        "\n",
        "        self.return_unprocessed_image = False\n",
        "        self.style_tf_fn = None\n",
        "\n",
        "        with open(os.path.join(self.root, \"train.txt\"), \"r\") as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        # manipulate each file row in order to obtain the correct path\n",
        "        self.paths_images = [l.strip() for l in lines]\n",
        "\n",
        "        self.len = len(self.paths_images)\n",
        "\n",
        "    def set_style_tf_fn(self, style_tf_fn):\n",
        "        self.style_tf_fn = style_tf_fn\n",
        "\n",
        "    def reset_style_tf_fn(self):\n",
        "        self.style_tf_fn = None\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x_path = os.path.join(self.root, \"images\", self.paths_images[index])\n",
        "        y_path = os.path.join(self.root, \"labels\", self.paths_images[index])\n",
        "\n",
        "        x = Image.open(x_path)\n",
        "        y = Image.open(y_path)\n",
        "\n",
        "        if self.return_unprocessed_image:\n",
        "            return x\n",
        "\n",
        "        if self.style_tf_fn is not None:\n",
        "            x = self.style_tf_fn(x)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            x, y = self.transform(x, y)\n",
        "        y = self.target_transform(y)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __map_labels(self):\n",
        "        mapping = np.zeros((256,), dtype=np.int64) + 255\n",
        "        for k, v in self.labels2train.items():\n",
        "            mapping[k] = v\n",
        "        return lambda x: from_numpy(mapping[x])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9nJvrc96-9j"
      },
      "source": [
        "### Validation Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sFM8MKzU6-9j"
      },
      "outputs": [],
      "source": [
        "colors = [\n",
        "        [128, 64, 128],\n",
        "        [244, 35, 232],\n",
        "        [70, 70, 70],\n",
        "        [102, 102, 156],\n",
        "        [190, 153, 153],\n",
        "        [153, 153, 153],\n",
        "        [250, 170, 30],\n",
        "        [220, 220, 0],\n",
        "        [107, 142, 35],\n",
        "        [152, 251, 152],\n",
        "        [0, 130, 180],\n",
        "        [220, 20, 60],\n",
        "        [255, 0, 0],\n",
        "        [0, 0, 142],\n",
        "        [0, 0, 70],\n",
        "        [0, 60, 100],\n",
        "        [0, 80, 100],\n",
        "        [0, 0, 230],\n",
        "        [119, 11, 32]    \n",
        "        ]\n",
        "\n",
        "label_colours = dict(zip(range(NUM_CLASSES), colors))\n",
        "\n",
        "def decode_segmap(temp):\n",
        "    #convert gray scale to color\n",
        "    #print colored map\n",
        "    temp=temp.numpy()\n",
        "    r = temp.copy()\n",
        "    g = temp.copy()\n",
        "    b = temp.copy()\n",
        "    for l in range(0, NUM_CLASSES):\n",
        "        r[temp == l] = label_colours[l][0]\n",
        "        g[temp == l] = label_colours[l][1]\n",
        "        b[temp == l] = label_colours[l][2]\n",
        "\n",
        "    rgb = np.zeros((temp.shape[0], temp.shape[1], 3))\n",
        "    rgb[:, :, 0] = r / 255.0\n",
        "    rgb[:, :, 1] = g / 255.0\n",
        "    rgb[:, :, 2] = b / 255.0\n",
        "    return rgb\n",
        "\n",
        "def compute_miou(net, val_dataloader):\n",
        "    net = net.to(DEVICE)\n",
        "    net.train(False)  # Set Network to evaluation mode\n",
        "    jaccard = MulticlassJaccardIndex(num_classes=NUM_CLASSES, ignore_index=255).to(\n",
        "        DEVICE\n",
        "    )\n",
        "\n",
        "    jacc = 0\n",
        "    count = 0\n",
        "    for images, labels in val_dataloader:\n",
        "        images = images.to(DEVICE, dtype=torch.float32)\n",
        "        labels = labels.to(DEVICE, dtype=torch.long)\n",
        "        # Forward Pass\n",
        "        outputs = net(images)\n",
        "        # Get predictions\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Update Corrects\n",
        "        jacc += jaccard(preds, labels.squeeze())\n",
        "        count += 1\n",
        "\n",
        "    # Calculate Accuracy\n",
        "    metric = jacc.item() / count\n",
        "    # net.train(True)\n",
        "    return metric\n",
        "\n",
        "\n",
        "def validation_plot(net, val_dataloader, n_images):\n",
        "    net = net.to(DEVICE)\n",
        "    net.train(False)\n",
        "    rows = 1\n",
        "    columns = 3\n",
        "    n = 0\n",
        "    for imgs, targets in val_dataloader:\n",
        "        # i = random.randint(BATCH_SIZE)\n",
        "        if n >= n_images:\n",
        "          break\n",
        "        imgsfloat = imgs.to(DEVICE, dtype=torch.float32)\n",
        "        outputs = net(imgsfloat)\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        # Added in order to use the decode_segmap function\n",
        "        preds = preds.cpu()  # or equally preds = preds.to('cpu')\n",
        "        \n",
        "        for i in range(len(imgs)):\n",
        "          if n >= n_images:\n",
        "            break\n",
        "          # pick the first image of each batch\n",
        "          print(imgs[i].shape, targets[i].shape)\n",
        "          print(\"img:\", imgs[i].squeeze().shape, \" target:\", targets[i].squeeze().shape)\n",
        "          print(\"pred:\", preds.shape)\n",
        "\n",
        "          figure = plt.figure(figsize=(10, 20))\n",
        "          figure.add_subplot(rows, columns, 1)\n",
        "          #plt.imshow(imgs[0].permute((1, 2, 0)).squeeze())\n",
        "          plt.imshow(imgs[i].permute((1, 2, 0)).squeeze())\n",
        "          plt.axis(\"off\")\n",
        "          plt.title(\"Image\")\n",
        "\n",
        "          figure.add_subplot(rows, columns, 2)\n",
        "          #plt.imshow(decode_segmap(targets[0].permute((1, 2, 0)).squeeze()))\n",
        "          plt.imshow(decode_segmap(targets[i]))\n",
        "          plt.axis(\"off\")\n",
        "          plt.title(\"Groundtruth\")\n",
        "\n",
        "          figure.add_subplot(rows, columns, 3)\n",
        "          #plt.imshow(decode_segmap(preds[0].squeeze()))\n",
        "          plt.imshow(decode_segmap(preds[i]))\n",
        "          plt.axis(\"off\")\n",
        "          plt.title(\"Prediction\")\n",
        "          plt.show()\n",
        "          n += 1\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP86iZXo6-9j"
      },
      "source": [
        "## STEP 1 : GENERATING DATASETS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4xxOV976-9j"
      },
      "source": [
        "### Generate splits for step 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5QnoyjK6-9j"
      },
      "outputs": [],
      "source": [
        "\n",
        "IMAGES_FINAL = \"leftImg8bit\"\n",
        "TARGET_FINAL = \"gtFine_labelIds\"\n",
        "\n",
        "with open(os.path.join(ROOT_DIR, \"train.txt\"), \"r\") as ft:\n",
        "    lines_train = ft.readlines()\n",
        "with open(os.path.join(ROOT_DIR, \"val.txt\"), \"r\") as fv:\n",
        "    lines_val = fv.readlines()\n",
        "\n",
        "lines = lines_train + lines_val\n",
        "images = [\n",
        "    (\n",
        "        l.split(\"/\")[0],\n",
        "        l.strip().split(\"/\")[1],\n",
        "        l.strip().split(\"/\")[1].replace(IMAGES_FINAL, TARGET_FINAL),\n",
        "    )\n",
        "    for l in lines\n",
        "]\n",
        "\n",
        "with open(os.path.join(ROOT_DIR, \"train_B.txt\"), \"w\") as f:\n",
        "    for l in lines_train:\n",
        "        img = l.strip().split(\"/\")[1]\n",
        "        lbl = img.replace(IMAGES_FINAL, TARGET_FINAL)\n",
        "        f.write(img + \"@\" + lbl + \"\\n\")\n",
        "\n",
        "with open(os.path.join(ROOT_DIR, \"test_B.txt\"), \"w\") as f:\n",
        "    for l in lines_val:\n",
        "        img = l.strip().split(\"/\")[1]\n",
        "        lbl = img.replace(IMAGES_FINAL, TARGET_FINAL)\n",
        "        f.write(img + \"@\" + lbl + \"\\n\")\n",
        "\n",
        "city_dic = {}\n",
        "\n",
        "for i in images:\n",
        "    if i[0] not in city_dic:\n",
        "        city_dic[i[0]] = []\n",
        "        city_dic[i[0]].append(tuple(i[1:]))\n",
        "    else:\n",
        "        city_dic[i[0]].append(tuple(i[1:]))\n",
        "test_img = []\n",
        "train_img = []\n",
        "\n",
        "for c in city_dic.values():\n",
        "    s = random.sample(c, 2)\n",
        "    test_img += s\n",
        "    train_img += c\n",
        "    for img in s:\n",
        "        train_img.remove(img)\n",
        "\n",
        "# save the split\n",
        "with open(os.path.join(ROOT_DIR, \"test_A.txt\"), \"w\") as f:\n",
        "    for img in test_img:\n",
        "        f.write(img[0] + \"@\" + img[1] + \"\\n\")\n",
        "\n",
        "with open(os.path.join(ROOT_DIR, \"train_A.txt\"), \"w\") as f:\n",
        "    for img in train_img:\n",
        "        f.write(str(img[0]) + \"@\" + img[1] + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in-y4BQ16-9k"
      },
      "source": [
        "### generating splits for step 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkmhhPbi6-9k"
      },
      "outputs": [],
      "source": [
        "if PARTITION == \"A\":\n",
        "    # train A\n",
        "    with open(os.path.join(ROOT_DIR, \"train_A.txt\"), \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "        images = [\n",
        "            (\n",
        "                l.strip().split(\"@\")[0],\n",
        "                l.strip().split(\"@\")[1],\n",
        "            )\n",
        "            for l in lines\n",
        "        ]\n",
        "if PARTITION == \"B\":\n",
        "    # train B\n",
        "    with open(os.path.join(ROOT_DIR, \"train.txt\"), \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "        images = [\n",
        "            (\n",
        "                l.strip().split(\"/\")[1],\n",
        "                l.strip().split(\"/\")[1].replace(IMAGES_FINAL, TARGET_FINAL),\n",
        "            )\n",
        "            for l in lines\n",
        "        ]\n",
        "\n",
        "city_dic = {}\n",
        "for i in images:\n",
        "    city_name = i[0].split(\"_\")[0]\n",
        "    if city_name not in city_dic:\n",
        "        city_dic[city_name] = []\n",
        "    city_dic[city_name].append(i)\n",
        "\n",
        "if SPLIT == 1:\n",
        "    # uniform\n",
        "    # every client has images from different cityes\n",
        "    n_sample = len(images)\n",
        "    n_client_per_city = math.ceil(n_sample / MAX_SAMPLE_PER_CLIENT)\n",
        "    city_enum = list(enumerate(city_dic.keys()))\n",
        "    choices = [k for k in city_dic.keys()]\n",
        "    weights = [len(city_dic[c]) for c in choices]\n",
        "    client_dict = {}\n",
        "    for i in range(n_client_per_city):\n",
        "        client_dict[i] = []\n",
        "        for _ in range(MAX_SAMPLE_PER_CLIENT):\n",
        "            choices = [k for k in city_dic.keys()]\n",
        "            weights = [len(city_dic[c]) for c in choices]\n",
        "            try:\n",
        "                c = random.choices(choices, weights=weights, k=1)[0]\n",
        "            except:\n",
        "                break\n",
        "            img, lable = city_dic[c].pop()\n",
        "            client_dict[i].append((img, lable))\n",
        "            if len(city_dic[c]) == 0:\n",
        "                city_dic.pop(c)\n",
        "\n",
        "    with open(\n",
        "        os.path.join(ROOT_DIR, f\"uniform{PARTITION}.json\"), \"w\"\n",
        "    ) as outfile:\n",
        "        json.dump(client_dict, outfile, indent=4)\n",
        "\n",
        "if SPLIT == 2:\n",
        "    # heterogeneous\n",
        "    # every client has images from only one city\n",
        "    client_dict = {}\n",
        "    tot_clients = 0\n",
        "    for city in city_dic.keys():\n",
        "        n_samples_per_city = len(city_dic[city])\n",
        "        n_client_per_city = math.ceil(n_samples_per_city / MAX_SAMPLE_PER_CLIENT)\n",
        "        avg = len(city_dic[city]) // n_client_per_city\n",
        "\n",
        "        for i in range(tot_clients, tot_clients + n_client_per_city):\n",
        "            client_dict[i] = []\n",
        "            for _ in range(avg):\n",
        "                img, lbl = city_dic[city].pop()\n",
        "                client_dict[i].append((img, lbl))\n",
        "            tot_clients += 1\n",
        "        if len(city_dic[city]) > 0:\n",
        "            for img, lbl in city_dic[city]:\n",
        "                client_dict[i].append((img, lbl))\n",
        "    with open(\n",
        "        os.path.join(\n",
        "            ROOT_DIR , f\"heterogeneuos{PARTITION}.json\"\n",
        "        ),\n",
        "        \"w\",\n",
        "    ) as outfile:\n",
        "        json.dump(client_dict, outfile, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4SzVD3y6-9k"
      },
      "source": [
        "## STEP 2 : CENTRALIZED BASELINE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMOIqK0E6-9k"
      },
      "source": [
        "### Setup for WanDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXb1eyGP6-9k"
      },
      "outputs": [],
      "source": [
        "wandb.login()\n",
        "\n",
        "transformer_dictionary = {\n",
        "    \"random-horizontal-flip\":RANDOM_HORIZONTAL_FLIP,\n",
        "    \"color-jitter\":COLOR_JITTER,\n",
        "    \"random-rotation\":RANDOM_ROTATION,\n",
        "    \"random-crop\":RANDOM_CROP,\n",
        "    \"random-vertical-flip\":RANDOM_VERTICAL_FLIP,\n",
        "    \"central-crop\":CENTRAL_CROP,\n",
        "    \"random-resized-crop\":RANDOM_RESIZE_CROP,\n",
        "    \"resize\":RESIZE,\n",
        "    }\n",
        "\n",
        "config = {\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"lr\": LR,\n",
        "        \"num_epochs\": NUM_EPOCHS,\n",
        "        #\"step_size\": STEP_SIZE,\n",
        "        \"transformers\": transformer_dictionary,\n",
        "    }\n",
        "name = f\"Step_2_{PARTITION}_lr{LR}_bs{BATCH_SIZE}_e{NUM_EPOCHS}\"\n",
        "wandb.init(\n",
        "    project = \" STEP2\",\n",
        "    # entity = \"lor-bellino\",\n",
        "    config = config,\n",
        "    name = name,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thOe5E9L6-9k"
      },
      "source": [
        "###  Creating Transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpUtUWvn6-9l"
      },
      "outputs": [],
      "source": [
        "transforms = setup_transform()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmj4pU7g6-9l"
      },
      "source": [
        "### Creating Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc4bjdqB6-9l"
      },
      "outputs": [],
      "source": [
        "\n",
        "if PARTITION == \"A\":\n",
        "    train_dataset = Cityscapes(\n",
        "        root=ROOT_DIR,\n",
        "        transform=transforms,\n",
        "        cl19=True,\n",
        "        filename=\"train_A.txt\",\n",
        "    )\n",
        "elif PARTITION == \"B\":\n",
        "    train_dataset = Cityscapes(\n",
        "        root=ROOT_DIR,\n",
        "        transform=transforms,\n",
        "        cl19=True,\n",
        "        filename=\"train_B.txt\",\n",
        "    )\n",
        "else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    drop_last=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT5FzsWo6-9l"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09N-OsD16-9l"
      },
      "outputs": [],
      "source": [
        "model = BiSeNetV2(NUM_CLASSES, output_aux=False, pretrained=True)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "parameters_to_optimize = model.parameters()\n",
        "optimizer = optim.SGD(\n",
        "    parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "cudnn.benchmark  # Calling this optimizes runtime\n",
        "\n",
        "epochs = []\n",
        "\n",
        "wandb.watch(model, log=\"all\")\n",
        "\n",
        "current_step = 0\n",
        "# Start iterating over the epochs\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(\"Starting epoch {}/{}\".format(epoch + 1, NUM_EPOCHS))\n",
        "    epochs.append(epoch + 1)\n",
        "\n",
        "    # Iterate over the dataset\n",
        "    for images, labels in train_dataloader:\n",
        "        images = images.to(DEVICE, dtype=torch.float32)\n",
        "        labels = labels.to(DEVICE, dtype=torch.long)\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predictions = model(images)\n",
        "        loss = criterion(predictions, labels.squeeze())\n",
        "\n",
        "        # Log loss\n",
        "        if current_step % LOG_FREQUENCY == 0:\n",
        "            print(\"Step {}, Loss {}\".format(current_step, loss.item()))\n",
        "            wandb.log({\"train/loss\": loss})\n",
        "        # Compute gradients for each layer and update weights\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        current_step += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6M0gTTV6-9l"
      },
      "source": [
        "### Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6g6lZev6-9l"
      },
      "outputs": [],
      "source": [
        "name = f\"step2_{PARTITION}_model.pth\"\n",
        "if not os.path.exists(\"./models/STEP2/\"):\n",
        "    print(\"creating models directory\")\n",
        "    os.makedirs(\"./models/STEP2/\")\n",
        "torch.save(model.state_dict(), \"./models/STEP2/\" + name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgyhY4as6-9l"
      },
      "source": [
        "### Creating Validation Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efioG6jY6-9l"
      },
      "outputs": [],
      "source": [
        "if PARTITION == \"A\":\n",
        "    val_dataset = Cityscapes(\n",
        "        root=ROOT_DIR,\n",
        "        transform=transforms,\n",
        "        cl19=True,\n",
        "        filename=\"test_A.txt\",\n",
        "    )\n",
        "elif PARTITION == \"B\":\n",
        "    val_dataset = Cityscapes(\n",
        "        root=ROOT_DIR,\n",
        "        transform=transforms,\n",
        "        cl19=True,\n",
        "        filename=\"test_B.txt\",\n",
        "    )\n",
        "else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3srK9CV6-9m"
      },
      "source": [
        "### Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IlpmO426-9m"
      },
      "outputs": [],
      "source": [
        "print(\"computing miou ...\")\n",
        "miou = compute_miou(net=model, val_dataloader=val_dataloader)\n",
        "print(\"Validation MIoU: {}\".format(miou))\n",
        "wandb.log({\"val/miou\": miou})\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4Y3nDTY6-9m"
      },
      "source": [
        "### Validation plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7htSRB-f6-9m"
      },
      "outputs": [],
      "source": [
        "print(\"validation plot : \")\n",
        "validation_plot(net=model, val_dataloader=val_dataloader, n_images=20)\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-V9jn1M6-9m"
      },
      "source": [
        "## STEP 3 : FEDERATED + SEMANTIC SEGMENTATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGq1UDX76-9m"
      },
      "source": [
        "### WanDB Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "toXv3-Kh6-9m",
        "outputId": "2aa874dc-f4b9-49a5-9318-bb3833d56c12"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlor-bellino\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/AMLproject/wandb/run-20230404_172051-cqsg7xhr</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lor-bellino/STEP3/runs/cqsg7xhr' target=\"_blank\">Step_3_B_S1_R50_c5</a></strong> to <a href='https://wandb.ai/lor-bellino/STEP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/lor-bellino/STEP3' target=\"_blank\">https://wandb.ai/lor-bellino/STEP3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/lor-bellino/STEP3/runs/cqsg7xhr' target=\"_blank\">https://wandb.ai/lor-bellino/STEP3/runs/cqsg7xhr</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/lor-bellino/STEP3/runs/cqsg7xhr?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f7338927310>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login()\n",
        "config = {\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"lr\": LR,\n",
        "        \"momentum\": MOMENTUM,\n",
        "        \"num_epochs\": NUM_EPOCHS,\n",
        "        \"n_client\": CLIENT_PER_ROUND,\n",
        "        \"round\": N_ROUND,\n",
        "        \"tot_client\": TOT_CLIENTS,\n",
        "        }\n",
        "name = (\n",
        "        f\"Step_3_{PARTITION}_S{SPLIT}_R{N_ROUND}_c{CLIENT_PER_ROUND}\"\n",
        "        )\n",
        "wandb.init(\n",
        "        project=f\"STEP3\",\n",
        "        # entity=\"lor-bellino\",\n",
        "        config=config,\n",
        "        name=name,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FubAoVS46-9m"
      },
      "source": [
        "### Client Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MdL2CVH6-9m"
      },
      "outputs": [],
      "source": [
        "class Client():\n",
        "  #def __init__(self, client_id, dataset, model, logger, writer, args, batch_size, world_size, rank, device=None, **kwargs):\n",
        "  def __init__(self, client_id, dataset, model):\n",
        "    self.id = client_id\n",
        "    self.dataset = dataset\n",
        "    self.model = model #copy.deepcopy(model)\n",
        "    self.device = DEVICE\n",
        "    self.batch_size = BATCH_SIZE\n",
        "    #self.args = args\n",
        "    self.loader = DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
        "\n",
        "\n",
        "  def client_train(self):\n",
        "    \n",
        "    num_train_samples = len(self.dataset)\n",
        "    # Define loss function\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index= 255)\n",
        "    parameters_to_optimize = self.model.parameters() \n",
        "    optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    self.model = self.model.to(DEVICE)\n",
        "    self.model.train() # Sets module in training mode\n",
        "\n",
        "    cudnn.benchmark # Calling this optimizes runtime\n",
        "    \n",
        "    # Start iterating over the epochs\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "      if epoch % LOG_FREQUENCY_EPOCH == 0: \n",
        "        print('Starting epoch {}/{}'.format(epoch+1, NUM_EPOCHS))\n",
        "\n",
        "      # Iterate over the dataset\n",
        "      for current_step, (images, labels) in enumerate(self.loader):\n",
        "        images = images.to(DEVICE, dtype=torch.float32)\n",
        "        labels = labels.to(DEVICE, dtype=torch.long)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        predictions = self.model(images)\n",
        "        loss = criterion(predictions, labels.squeeze())\n",
        "\n",
        "        # Log loss\n",
        "        if current_step % LOG_FREQUENCY == 0:\n",
        "          print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "          #wandb.log({f\"client{self.id}/loss\":loss})\n",
        "          wandb.log({f\"client/loss\":loss})\n",
        "\n",
        "        loss.backward()  # backward pass: computes gradients\n",
        "        optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "    return num_train_samples, copy.deepcopy(self.model.state_dict()) #generate_update\n",
        "\n",
        "  def test(self, metrics, ret_samples_ids=None, silobn_type=None, train_cl_bn_stats=None, loader=None):\n",
        "    return\n",
        "\n",
        "  def save_model(self, epochs, path, optimizer, scheduler):\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8fGEHjN6-9n"
      },
      "source": [
        "### Server Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEjqkI2S6-9n"
      },
      "outputs": [],
      "source": [
        "class Server():\n",
        "  #def __init__(self, model, logger, writer, local_rank, lr, momentum, optimizer=None):\n",
        "  def __init__(self, model, lr= None , momentum= None):\n",
        "    self.model = copy.deepcopy(model)\n",
        "    self.model_params_dict = copy.deepcopy(self.model.state_dict())\n",
        "    self.selected_clients = []\n",
        "    self.updates = []\n",
        "    self.lr = lr\n",
        "    self.momentum = momentum\n",
        "    self.optimizer = optim.SGD(params=self.model.parameters(), lr=1, momentum=0.9)\n",
        "    self.total_grad = 0\n",
        "\n",
        "  def select_clients(self, my_round, possible_clients, num_clients=4):\n",
        "    num_clients = min(num_clients, len(possible_clients))\n",
        "    np.random.seed(my_round)\n",
        "    self.selected_clients = np.random.choice(possible_clients, num_clients, replace=False)\n",
        "  \n",
        "  def _compute_client_delta(self, cmodel):\n",
        "      delta = OrderedDict.fromkeys(cmodel.keys())\n",
        "      for k, x, y in zip(self.model_params_dict.keys(), self.model_params_dict.values(), cmodel.values()):\n",
        "        delta[k] = y - x if \"running\" not in k and \"num_batches_tracked\" not in k else y\n",
        "      return delta\n",
        "  \n",
        "  def train_round(self):\n",
        "\n",
        "    self.optimizer.zero_grad()\n",
        "\n",
        "    clients = self.selected_clients\n",
        "\n",
        "    for i, c in enumerate(clients):\n",
        "\n",
        "        print(f\"CLIENT {i + 1}/{len(clients)} -> {c.id}:\")\n",
        "\n",
        "        c.model.load_state_dict(self.model_params_dict) # load_server_model_on_client\n",
        "        out = c.client_train()\n",
        "\n",
        "        num_samples, update = out\n",
        "\n",
        "        update = self._compute_client_delta(update)\n",
        "        \n",
        "        self.updates.append((num_samples, update))\n",
        "    return \n",
        "\n",
        "  def _server_opt(self, pseudo_gradient):\n",
        "    for n, p in self.model.named_parameters():\n",
        "        p.grad = -1.0 * pseudo_gradient[n]\n",
        "\n",
        "    self.optimizer.step()\n",
        "\n",
        "    bn_layers = OrderedDict(\n",
        "        {k: v for k, v in pseudo_gradient.items() if \"running\" in k or \"num_batches_tracked\" in k})\n",
        "    self.model.load_state_dict(bn_layers, strict=False)\n",
        "\n",
        "  def _aggregation(self):\n",
        "    total_weight = 0.\n",
        "    base = OrderedDict()\n",
        "\n",
        "    for (client_samples, client_model) in self.updates:\n",
        "\n",
        "        total_weight += client_samples\n",
        "        for key, value in client_model.items():\n",
        "            if key in base:\n",
        "                base[key] += client_samples * value.type(torch.FloatTensor)\n",
        "            else:\n",
        "                base[key] = client_samples * value.type(torch.FloatTensor)\n",
        "    averaged_sol_n = copy.deepcopy(self.model_params_dict)\n",
        "    for key, value in base.items():\n",
        "        if total_weight != 0:\n",
        "            averaged_sol_n[key] = value.to('cuda') / total_weight\n",
        "\n",
        "    return averaged_sol_n\n",
        "\n",
        "  def _get_model_total_grad(self):\n",
        "    total_norm = 0\n",
        "    for name, p in self.model.named_parameters():\n",
        "        if p.requires_grad:\n",
        "            param_norm = p.grad.data.norm(2)\n",
        "            total_norm += param_norm.item() ** 2\n",
        "    total_grad = total_norm ** 0.5\n",
        "    return total_grad\n",
        "\n",
        "  def update_model(self):\n",
        "    \"\"\"FedAvg on the clients' updates for the current round.\n",
        "    Weighted average of self.updates, where the weight is given by the number\n",
        "    of samples seen by the corresponding client at training time.\n",
        "    Saves the new central model in self.client_model and its state dictionary in self.model\n",
        "    \"\"\"\n",
        "\n",
        "    averaged_sol_n = self._aggregation()\n",
        "    \n",
        "    self._server_opt(averaged_sol_n)\n",
        "    self.total_grad = self._get_model_total_grad()\n",
        "    self.model_params_dict = copy.deepcopy(self.model.state_dict())\n",
        "\n",
        "    self.updates = []\n",
        "\n",
        "  def test_model(self, clients_to_test, metrics, ret_samples_bool=False, silobn_type=''):\n",
        "    return \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2m0FDD9A6-9n"
      },
      "source": [
        "### Creating Transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYpjfQBz6-9n"
      },
      "outputs": [],
      "source": [
        "transforms = setup_transform()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kcwj25X6-9n"
      },
      "source": [
        "### Client Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-NqUSn96-9n"
      },
      "outputs": [],
      "source": [
        "def setup_clients(n_client, model):\n",
        "\n",
        "  clients = []\n",
        "  if PARTITION == 'A':\n",
        "    if SPLIT == 1:\n",
        "      filename=\"uniformA.json\"\n",
        "    else:\n",
        "      filename='heterogeneuosA.json'\n",
        "    for i in range(n_client):\n",
        "      train_dataset = Cityscapes(root=ROOT_DIR, transform=transforms, cl19 = cl19,filename=filename,  id_client = i)\n",
        "      client = Client(client_id = i, dataset = train_dataset, model = model)\n",
        "      clients.append(client)\n",
        "  else:\n",
        "    if SPLIT == 1:\n",
        "      filename=\"uniformB.json\"\n",
        "    else:\n",
        "      filename='heterogeneuosB.json'\n",
        "    for i in range(n_client):\n",
        "      train_dataset = Cityscapes(root=ROOT_DIR, transform=transforms, cl19 = cl19, filename=filename,  id_client = i)\n",
        "      client = Client(client_id = i, dataset = train_dataset, model = model)\n",
        "      clients.append(client)\n",
        "\n",
        "  return clients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICF_u3x46-9n"
      },
      "source": [
        "### Validation Dataloader between rounds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cXdmmAP6-9n"
      },
      "outputs": [],
      "source": [
        "def create_val_dataloader(transforms):\n",
        "    if SPLIT == 1:\n",
        "        filename = f\"uniform{PARTITION}.json\"\n",
        "    else:\n",
        "        filename = f\"heterogeneuos{PARTITION}.json\"\n",
        "\n",
        "    val_dataset = Cityscapes(\n",
        "        root=ROOT_DIR,\n",
        "        transform=transforms,\n",
        "        cl19=cl19,\n",
        "        filename=filename,\n",
        "        id_client=0,\n",
        "    )\n",
        "    val_dataloader = DataLoader(\n",
        "        val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=True\n",
        "    )\n",
        "    return val_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1eYeIaC6-9n"
      },
      "source": [
        "### Server Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGWQk5156-9n"
      },
      "outputs": [],
      "source": [
        "model = BiSeNetV2(NUM_CLASSES, output_aux=False, pretrained=True)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "wandb.watch(model, log='all')\n",
        "\n",
        "model_path = \"./models/Step3/\"\n",
        "if not os.path.exists(model_path):\n",
        "    print(\"creating models directory\")\n",
        "    os.makedirs(model_path)\n",
        "\n",
        "train_clients = setup_clients(n_client = TOT_CLIENTS, model = model)\n",
        "print('Number of total clients:', len(train_clients))\n",
        "val_dataloader = create_val_dataloader(transforms)\n",
        "\n",
        "server = Server(model, lr=LR, momentum = MOMENTUM)\n",
        "\n",
        "for r in range(N_ROUND):\n",
        "  print(f\"ROUND {r + 1}/{N_ROUND}: Training {CLIENT_PER_ROUND} Clients...\")\n",
        "  server.select_clients(r, train_clients, num_clients=CLIENT_PER_ROUND) \n",
        "  server.train_round()\n",
        "  server.update_model()\n",
        "  miou = compute_miou(net=server.model, val_dataloader=val_dataloader)\n",
        "  wandb.log({\"server/miou\": miou})\n",
        "  print(f\"Validation MIoU: {miou}\")\n",
        "  if r%CHECKPOINTS == 0:\n",
        "    print(f\"Saving the model\")\n",
        "    torch.save(model.state_dict(), \"./models/STEP3/\"+f\"model_P{PARTITION}_S{SPLIT}_round{r:02}.pth\")\n",
        "\n",
        "torch.save(model.state_dict(), \"./models/STEP3/\"+f\"model_P{PARTITION}_S{SPLIT}_round{r:02}.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utDVRnZ36-9o"
      },
      "source": [
        "### Creating Validation Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agpan0_06-9o",
        "outputId": "4fc6bb7a-8014-4297-b347-ccf5c691991f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset dimension:  250\n"
          ]
        }
      ],
      "source": [
        "if PARTITION == 'A': \n",
        "  filename=\"test_A.txt\"\n",
        "elif PARTITION == 'B': \n",
        "  filename=\"test_B.txt\"\n",
        "\n",
        "val_dataset = Cityscapes(root=ROOT_DIR,transform=transforms, cl19 = cl19, filename=filename)\n",
        "print(\"Dataset dimension:\", len(val_dataset))\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlgZMIdS6-9o"
      },
      "source": [
        "### Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345,
          "referenced_widgets": [
            "c7ccda91edf7486f9bad5100ec2ede38",
            "6b5fce34081348e6bf995d1b2640708d",
            "85c1ed2d3af64782813a5dacb09d7f1d",
            "98b86aed00b94600b928ed49224501da",
            "32ee5d0ce27d429cb2aa51812267d836",
            "9e6f29242e9b47b7bddd20ae5677abf8",
            "f82a991db17e485cad5e8a11c6641ea2",
            "91025867a63f4ef9b5cc1fd54cc6c9ca"
          ]
        },
        "id": "dRBURp-k6-9o",
        "outputId": "f9f501ca-2524-4aa7-968c-651441d42b8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "computing miou ...\n",
            "Validation MIoU: 0.2096177839463757\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c7ccda91edf7486f9bad5100ec2ede38",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>client/loss</td><td>█▇▆▄▃▃▃▃▂▂▃▃▂▂▂▂▂▁▂▂▁▂▂▂▂▁▁▂▂▁▁▂▂▁▂▂▁▁▁▁</td></tr><tr><td>server/miou</td><td>▁▂▂▃▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇██████████████</td></tr><tr><td>val/miou</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>client/loss</td><td>0.46031</td></tr><tr><td>server/miou</td><td>0.20814</td></tr><tr><td>val/miou</td><td>0.20962</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Step_3_B_S1_R50_c5</strong> at: <a href='https://wandb.ai/lor-bellino/STEP3/runs/cqsg7xhr' target=\"_blank\">https://wandb.ai/lor-bellino/STEP3/runs/cqsg7xhr</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230404_172051-cqsg7xhr/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"computing miou ...\")\n",
        "miou = compute_miou(net=model, val_dataloader=val_dataloader)\n",
        "print(\"Validation MIoU: {}\".format(miou))\n",
        "wandb.log({\"val/miou\": miou})\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9Qr-Riq6-9o"
      },
      "source": [
        "### Validation Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFJnZlJ1QvT5"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(),\"./models/model_S3_{PARTITION}.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIvyRLqW6-9o",
        "outputId": "a93245fd-d51b-479d-8b26-00a0e2179334"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation plot : \n"
          ]
        }
      ],
      "source": [
        "print(\"validation plot : \")\n",
        "validation_plot(net=model, val_dataloader=val_dataloader, n_images=20)\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCZd4PE9pHKm"
      },
      "source": [
        "## STEP 4 : MOVING TOWARDS FREEDA - PRETRAINING PHASE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1E3IwIApHKm"
      },
      "source": [
        "### Setup for WanDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "GRTz7OP-pHKm",
        "outputId": "cb20fedf-6b7a-4d0c-b530-567a336a54c2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/AMLproject/wandb/run-20230406_151129-klbi2lxe</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aml2023/%20STEP4.4/runs/klbi2lxe' target=\"_blank\">Step_4_FDA_heterogeneuosA.json_lr0.01_bs10_e10</a></strong> to <a href='https://wandb.ai/aml2023/%20STEP4.4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aml2023/%20STEP4.4' target=\"_blank\">https://wandb.ai/aml2023/%20STEP4.4</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aml2023/%20STEP4.4/runs/klbi2lxe' target=\"_blank\">https://wandb.ai/aml2023/%20STEP4.4/runs/klbi2lxe</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/aml2023/%20STEP4.4/runs/klbi2lxe?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f19c855a940>"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login()\n",
        "\n",
        "transformer_dictionary = {\n",
        "    \"random-horizontal-flip\":RANDOM_HORIZONTAL_FLIP,\n",
        "    \"color-jitter\":COLOR_JITTER,\n",
        "    \"random-rotation\":RANDOM_ROTATION,\n",
        "    \"random-crop\":RANDOM_CROP,\n",
        "    \"random-vertical-flip\":RANDOM_VERTICAL_FLIP,\n",
        "    \"central-crop\":CENTRAL_CROP,\n",
        "    \"random-resized-crop\":RANDOM_RESIZE_CROP,\n",
        "    \"resize\":RESIZE,\n",
        "    }\n",
        "\n",
        "config = {\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"lr\": LR,\n",
        "        \"num_epochs\": NUM_EPOCHS,\n",
        "        \"weight_decay\": WEIGHT_DECAY,\n",
        "        \"momentum\" : MOMENTUM,\n",
        "        #\"step_size\": STEP_SIZE,\n",
        "        \"transformers\": transformer_dictionary,\n",
        "        \"FDA\" : FDA,\n",
        "        \"num_styles\": N_STYLE,\n",
        "        \"beta_window_size\": BETA_WINDOW_SIZE\n",
        "    }\n",
        "\n",
        "name = f\"step4{'_FDA' if FDA else ''}_{PARTITION}_S{SPLIT}_lr{LR}_bs{BATCH_SIZE}_e{NUM_EPOCHS}\"\n",
        "# if FDA:\n",
        "#   name = f\"Step_4_FDA_{PARTITION}_S{SPLIT}_lr{LR}_bs{BATCH_SIZE}_e{NUM_EPOCHS}\"\n",
        "# else: \n",
        "#   name = f\"Step_4_{PARTITION}_S{SPLIT}_lr{LR}_bs{BATCH_SIZE}_e{NUM_EPOCHS}\"\n",
        "\n",
        "wandb.init(\n",
        "    project = f\"STEP{'4.4' if FDA else '4.2' }\",\n",
        "    # entity = \"lor-bellino\",\n",
        "    config = config,\n",
        "    name = name,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUnvFJJypHKm"
      },
      "source": [
        "### Creating Transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0CG0AerpHKm"
      },
      "outputs": [],
      "source": [
        "transforms = setup_transform()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVhKMfJry4fp"
      },
      "source": [
        "### Style Augment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvTiJg-zthEe"
      },
      "outputs": [],
      "source": [
        "class StyleAugment:\n",
        "\n",
        "    def __init__(self, n_images_per_style=10, L=0.1, size=(1024, 512), b=None):\n",
        "        self.styles = []\n",
        "        self.styles_names = []\n",
        "        self.n_images_per_style = n_images_per_style\n",
        "        self.L = L\n",
        "        self.size = size\n",
        "        self.sizes = None\n",
        "        self.cv2 = False\n",
        "        self.b = b\n",
        "\n",
        "    def preprocess(self, x):\n",
        "        if isinstance(x, np.ndarray):\n",
        "            x = cv2.resize(x, self.size, interpolation=cv2.INTER_CUBIC)\n",
        "            self.cv2 = True\n",
        "        else:\n",
        "            x = x.resize(self.size, Image.BICUBIC)\n",
        "        x = np.asarray(x, np.float32)\n",
        "        x = x[:, :, ::-1]\n",
        "        x = x.transpose((2, 0, 1))\n",
        "        return x.copy()\n",
        "\n",
        "    def deprocess(self, x, size):\n",
        "        if self.cv2:\n",
        "            x = cv2.resize(np.uint8(x).transpose((1, 2, 0))[:, :, ::-1], size, interpolation=cv2.INTER_CUBIC)\n",
        "        else:\n",
        "            x = Image.fromarray(np.uint8(x).transpose((1, 2, 0))[:, :, ::-1])\n",
        "            x = x.resize(size, Image.BICUBIC)\n",
        "        return x\n",
        "\n",
        "    def add_style(self, loader, multiple_styles=False, name=None):\n",
        "        if self.n_images_per_style < 0:\n",
        "            return\n",
        "\n",
        "        if name is not None:\n",
        "            self.styles_names.append([name] * self.n_images_per_style if multiple_styles else [name])\n",
        "\n",
        "        loader.return_unprocessed_image = True\n",
        "        n = 0\n",
        "        styles = []\n",
        "\n",
        "        for sample in tqdm(loader, total=min(len(loader), self.n_images_per_style)):\n",
        "\n",
        "            image = self.preprocess(sample)\n",
        "\n",
        "            if n >= self.n_images_per_style:\n",
        "                break\n",
        "            styles.append(self._extract_style(image))\n",
        "            n += 1\n",
        "\n",
        "        if self.n_images_per_style > 1:\n",
        "            if multiple_styles:\n",
        "                self.styles += styles\n",
        "            else:\n",
        "                styles = np.stack(styles, axis=0)\n",
        "                style = np.mean(styles, axis=0)\n",
        "                self.styles.append(style)\n",
        "        elif self.n_images_per_style == 1:\n",
        "            self.styles += styles\n",
        "\n",
        "        loader.return_unprocessed_image = False\n",
        "\n",
        "    def _extract_style(self, img_np):\n",
        "        fft_np = np.fft.fft2(img_np, axes=(-2, -1))\n",
        "        amp = np.abs(fft_np)\n",
        "        amp_shift = np.fft.fftshift(amp, axes=(-2, -1))\n",
        "        if self.sizes is None:\n",
        "            self.sizes = self.compute_size(amp_shift)\n",
        "        h1, h2, w1, w2 = self.sizes\n",
        "        style = amp_shift[:, h1:h2, w1:w2]\n",
        "        return style\n",
        "\n",
        "    def compute_size(self, amp_shift):\n",
        "        _, h, w = amp_shift.shape\n",
        "        b = (np.floor(np.amin((h, w)) * self.L)).astype(int) if self.b is None else self.b\n",
        "        c_h = np.floor(h / 2.0).astype(int)\n",
        "        c_w = np.floor(w / 2.0).astype(int)\n",
        "        h1 = c_h - b\n",
        "        h2 = c_h + b + 1\n",
        "        w1 = c_w - b\n",
        "        w2 = c_w + b + 1\n",
        "        return h1, h2, w1, w2\n",
        "\n",
        "    def apply_style(self, image):\n",
        "        return self._apply_style(image)\n",
        "\n",
        "    def _apply_style(self, img):\n",
        "\n",
        "        if self.n_images_per_style < 0:\n",
        "            return img\n",
        "\n",
        "        if len(self.styles) > 0:\n",
        "            n = random.randint(0, len(self.styles) - 1)\n",
        "            style = self.styles[n]\n",
        "        else:\n",
        "            style = self.styles[0]\n",
        "\n",
        "        if isinstance(img, np.ndarray):\n",
        "            H, W = img.shape[0:2]\n",
        "        else:\n",
        "            W, H = img.size\n",
        "        img_np = self.preprocess(img)\n",
        "\n",
        "        fft_np = np.fft.fft2(img_np, axes=(-2, -1))\n",
        "        amp, pha = np.abs(fft_np), np.angle(fft_np)\n",
        "        amp_shift = np.fft.fftshift(amp, axes=(-2, -1))\n",
        "        h1, h2, w1, w2 = self.sizes\n",
        "        amp_shift[:, h1:h2, w1:w2] = style\n",
        "        amp_ = np.fft.ifftshift(amp_shift, axes=(-2, -1))\n",
        "\n",
        "        fft_ = amp_ * np.exp(1j * pha)\n",
        "        img_np_ = np.fft.ifft2(fft_, axes=(-2, -1))\n",
        "        img_np_ = np.real(img_np_)\n",
        "        img_np__ = np.clip(np.round(img_np_), 0., 255.)\n",
        "\n",
        "        img_with_style = self.deprocess(img_np__, (W, H))\n",
        "\n",
        "        return img_with_style\n",
        "\n",
        "    def test(self, images_np, images_target_np=None, size=None):\n",
        "\n",
        "        Image.fromarray(np.uint8(images_np.transpose((1, 2, 0)))[:, :, ::-1]).show()\n",
        "        fft_np = np.fft.fft2(images_np, axes=(-2, -1))\n",
        "        amp = np.abs(fft_np)\n",
        "        amp_shift = np.fft.fftshift(amp, axes=(-2, -1))\n",
        "        h1, h2, w1, w2 = self.sizes\n",
        "        style = amp_shift[:, h1:h2, w1:w2]\n",
        "\n",
        "        fft_np_ = np.fft.fft2(images_np if images_target_np is None else images_target_np, axes=(-2, -1))\n",
        "        amp_, pha_ = np.abs(fft_np_), np.angle(fft_np_)\n",
        "        amp_shift_ = np.fft.fftshift(amp_, axes=(-2, -1))\n",
        "        h1, h2, w1, w2 = self.sizes\n",
        "        amp_shift_[:, h1:h2, w1:w2] = style\n",
        "        amp__ = np.fft.ifftshift(amp_shift_, axes=(-2, -1))\n",
        "\n",
        "        fft_ = amp__ * np.exp(1j * pha_)\n",
        "        img_np_ = np.fft.ifft2(fft_, axes=(-2, -1))\n",
        "        img_np_ = np.real(img_np_)\n",
        "        img_np__ = np.clip(np.round(img_np_), 0., 255.)\n",
        "        Image.fromarray(np.uint8(images_target_np.transpose((1, 2, 0)))[:, :, ::-1]).show()\n",
        "        Image.fromarray(np.uint8(img_np__).transpose((1, 2, 0))[:, :, ::-1]).show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBK1fq8gpHKm"
      },
      "source": [
        "### Creating Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUToXb0Cu6al",
        "outputId": "52dc9df3-ce5a-40c2-86a9-550734d8c367"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [00:06<00:00,  2.44it/s]\n",
            "100%|██████████| 18/18 [00:07<00:00,  2.41it/s]\n",
            "100%|██████████| 16/16 [00:11<00:00,  1.45it/s]\n",
            "100%|██████████| 18/18 [00:12<00:00,  1.44it/s]\n",
            "100%|██████████| 17/17 [00:08<00:00,  1.94it/s]\n"
          ]
        }
      ],
      "source": [
        "# define the clients type\n",
        "if SPLIT == 1:\n",
        "  filename = f'uniform{PARTITION}.json'\n",
        "else:\n",
        "  filename = f'heterogeneuos{PARTITION}.json'\n",
        "  \n",
        "if FDA:\n",
        "  #L 0.01, 0.05, 0.09\n",
        "  # b == 0 --> 1x1, b == 1 --> 3x3, b == 2 --> 5x5, ...'\n",
        "  SA = StyleAugment(n_images_per_style=MAX_SAMPLE_PER_CLIENT, L=0.01, size=(1024, 512), b=BETA_WINDOW_SIZE) \n",
        "\n",
        "  clients = random.sample([_ for _ in range(TOT_CLIENTS)],N_STYLE)\n",
        "  for c in clients:\n",
        "    client_dataset = Cityscapes(root=CTSC_ROOT, transform=transforms, cl19 = cl19,filename=filename,  id_client = c)\n",
        "    SA.add_style(client_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBnREYVKpHKm"
      },
      "outputs": [],
      "source": [
        "train_dataset = GTA5(root=GTA5_ROOT, transform=transforms)\n",
        "if FDA:\n",
        "  train_dataset.set_style_tf_fn(SA.apply_style)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4bl6HbppHKn"
      },
      "source": [
        "### Creating MIOU dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuxE3AQkpHKn"
      },
      "outputs": [],
      "source": [
        "miou_filename = f\"uniform{PARTITION}.json\"\n",
        "miou_dataset = Cityscapes(root=CTSC_ROOT, transform=transforms, cl19 = cl19, filename=miou_filename, id_client = random.randint(0,TOT_CLIENTS-1))\n",
        "miou_dataloader = DataLoader(miou_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-nkULlbpHKn"
      },
      "source": [
        "### Loading the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i75aR07hpHKn"
      },
      "outputs": [],
      "source": [
        "if LOAD_CKPT:\n",
        "    print(\"loading the model\")\n",
        "    model = BiSeNetV2(NUM_CLASSES, output_aux=False, pretrained=False)\n",
        "    model.load_state_dict(torch.load(\"models/\" + LOAD_CKPT_PATH))\n",
        "else:\n",
        "    print(\"creating a new model\")\n",
        "    model = BiSeNetV2(NUM_CLASSES, output_aux=False, pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2fV3Ka_pHKn"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1ypIrI9pHKn",
        "outputId": "847bd239-258d-4c07-b95d-8a6bf345cde1"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "parameters_to_optimize = model.parameters()\n",
        "optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "cudnn.benchmark  # Calling this optimizes runtime\n",
        "\n",
        "epochs = []\n",
        "\n",
        "wandb.watch(model, log=\"all\")\n",
        "\n",
        "current_step = 0\n",
        "\n",
        "best_miou = 0\n",
        "\n",
        "# Start iterating over the epochs\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"Starting epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
        "    epochs.append(epoch + 1)\n",
        "    for images, labels in train_dataloader:\n",
        "       images = images.to(DEVICE, dtype=torch.float32)\n",
        "       labels = labels.to(DEVICE, dtype=torch.long)\n",
        "       model.train()\n",
        "       optimizer.zero_grad()\n",
        "\n",
        "       predictions = model(images)\n",
        "       loss = criterion(predictions, labels.squeeze())\n",
        "\n",
        "       # Log loss\n",
        "       if current_step % LOG_FREQUENCY == 0:\n",
        "           print(f\"Step {current_step}, Loss {loss.item()}\")\n",
        "           wandb.log({\"train/loss\": loss})\n",
        "        # Compute gradients for each layer and update weights\n",
        "       loss.backward()\n",
        "       optimizer.step()\n",
        "\n",
        "       current_step += 1\n",
        "    miou = compute_miou(model, miou_dataloader)\n",
        "    wandb.log({\"train/miou\": miou})\n",
        "    if miou > best_miou:\n",
        "        print(f\"Saving the model with miou {miou:.2} (best so far)\")\n",
        "        best_miou = miou\n",
        "        torch.save(\n",
        "               {\n",
        "                    \"epoch\": epoch + 1,\n",
        "                    \"model_state_dict\": model.state_dict(),\n",
        "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                    \"loss\": loss,\n",
        "                    \"miou\": miou,\n",
        "                },\n",
        "                CKPT_PATH + f\"step4_{PARTITION}_S{SPLIT}_{'FDA' if FDA else 'pretraining'}_{miou:.2}.pth\",\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rv1uaexKpHKn"
      },
      "source": [
        "### Creating Validation Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WX4OY0-zpHKn"
      },
      "outputs": [],
      "source": [
        "val_dataset = Cityscapes(root=CTSC_ROOT, transform=transforms, cl19 = cl19, filename=f\"test_{PARTITION}.txt\")\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4oRdKejDU5U"
      },
      "source": [
        "### Extract the best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zlDnMxQDU5U"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "ckpt = torch.load(CKPT_PATH + f\"step4_{PARTITION}_S{SPLIT}_{'FDA' if FDA else 'pretraining'}_{best_miou:.2}.pth\")\n",
        "model = BiSeNetV2(NUM_CLASSES, output_aux=False, pretrained=True)\n",
        "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
        "model.eval()\n",
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCcXy4nKpHKn"
      },
      "source": [
        "### Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347,
          "referenced_widgets": [
            "741f843e369442cea3acae6a256e630c",
            "64577185fcf3462986b957147ea263a5",
            "68ea1b36b1be4291a81fdd99cdc1d77b",
            "0e6e2a00f5b54c6b946780c026594b1f",
            "8dec5d9759984988ac05d56aadc8349a",
            "a8d0e49771b4498f9b1e7b6f8a6a4c8a",
            "125a6602f58447478603015a1d7e0895",
            "86298e6fbe7446c8b026105e87e467c9"
          ]
        },
        "id": "JP0pYOvVpHKn",
        "outputId": "f3c06069-f5a1-414d-a7be-8e319f68cb68"
      },
      "outputs": [],
      "source": [
        "print(\"computing miou ...\")\n",
        "miou = compute_miou(net=model, val_dataloader=val_dataloader)\n",
        "print(\"Validation MIoU: {}\".format(miou))\n",
        "wandb.log({\"val/miou\": miou})\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjyT3aV9pHKn"
      },
      "source": [
        "### Validation Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "id": "v5zgZ_FKpHKn",
        "outputId": "bb4e7ac2-0934-4c09-c6e1-6476b6a49486"
      },
      "outputs": [],
      "source": [
        "print(\"validation plot : \")\n",
        "validation_plot(net=model, val_dataloader=val_dataloader, n_images=5)\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H6WTJSU9lJRX"
      },
      "source": [
        "## STEP 5 : FEDERATED SELF TRAINING USING PSEUDO-LABELS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EzNL5fB3V_p"
      },
      "source": [
        "### Setup for WanDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "MPbzI5tu3WkW",
        "outputId": "7d5f00c5-67f1-481f-a93c-825b572a7f58"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33melia-fontana\u001b[0m (\u001b[33maml2023\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/AMLproject/wandb/run-20230408_133117-dluc62o6</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aml2023/STEP5/runs/dluc62o6' target=\"_blank\">Step_5_A_S1_R5_c5</a></strong> to <a href='https://wandb.ai/aml2023/STEP5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aml2023/STEP5' target=\"_blank\">https://wandb.ai/aml2023/STEP5</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aml2023/STEP5/runs/dluc62o6' target=\"_blank\">https://wandb.ai/aml2023/STEP5/runs/dluc62o6</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/aml2023/STEP5/runs/dluc62o6?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fee1fe0b4f0>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login()\n",
        "\n",
        "transformer_dictionary = {\n",
        "    \"random-horizontal-flip\":RANDOM_HORIZONTAL_FLIP,\n",
        "    \"color-jitter\":COLOR_JITTER,\n",
        "    \"random-rotation\":RANDOM_ROTATION,\n",
        "    \"random-crop\":RANDOM_CROP,\n",
        "    \"random-vertical-flip\":RANDOM_VERTICAL_FLIP,\n",
        "    \"central-crop\":CENTRAL_CROP,\n",
        "    \"random-resized-crop\":RANDOM_RESIZE_CROP,\n",
        "    \"resize\":RESIZE,\n",
        "    }\n",
        "\n",
        "config = {\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"lr\": LR,\n",
        "        \"momentum\": MOMENTUM,\n",
        "        \"weight_decay\": WEIGHT_DECAY,\n",
        "        \"num_epochs\": NUM_EPOCHS,\n",
        "        \"n_client\": CLIENT_PER_ROUND,\n",
        "        \"round\": N_ROUND,\n",
        "        \"tot_client\": TOT_CLIENTS,\n",
        "        \"transformers\": transformer_dictionary,\n",
        "    }\n",
        "\n",
        "\n",
        "name = f\"Step_5{'_FDA' if FDA else ''}_T{T_ROUND}_{PARTITION}_S{SPLIT}_R{N_ROUND}_c{CLIENT_PER_ROUND}\"\n",
        "\n",
        "wandb.init(\n",
        "    project = \"STEP5\",\n",
        "    # entity = \"lor-bellino\",\n",
        "    config = config,\n",
        "    name = name,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkW1GkMcFlmj"
      },
      "source": [
        "### Creating Transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "QiLUNP9KFkmv"
      },
      "outputs": [],
      "source": [
        "transforms = setup_transform()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kca9dJtv8dsz"
      },
      "source": [
        "### Self Training Loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "94bJa3CPulZc"
      },
      "outputs": [],
      "source": [
        "class SelfTrainingLoss(nn.Module):\n",
        "    requires_reduction = False\n",
        "\n",
        "    def __init__(self, conf_th=0.9, fraction=0.66, ignore_index=255, lambda_selftrain=1, **kwargs):\n",
        "        super().__init__()\n",
        "        self.conf_th = conf_th\n",
        "        self.fraction = fraction\n",
        "        self.ignore_index = ignore_index\n",
        "        self.teacher = None\n",
        "        self.lambda_selftrain = lambda_selftrain\n",
        "\n",
        "    def set_teacher(self, model):\n",
        "        self.teacher = model\n",
        "\n",
        "    def get_image_mask(self, prob, pseudo_lab):\n",
        "        max_prob = prob.detach().clone().max(0)[0]\n",
        "        mask_prob = max_prob > self.conf_th if 0. < self.conf_th < 1. else torch.zeros(max_prob.size(),\n",
        "                                                                                       dtype=torch.bool).to(\n",
        "            max_prob.device)\n",
        "        mask_topk = torch.zeros(max_prob.size(), dtype=torch.bool).to(max_prob.device)\n",
        "        if 0. < self.fraction < 1.:\n",
        "            for c in pseudo_lab.unique():\n",
        "                mask_c = pseudo_lab == c\n",
        "                max_prob_c = max_prob.clone()\n",
        "                max_prob_c[~mask_c] = 0\n",
        "                _, idx_c = torch.topk(max_prob_c.flatten(), k=int(mask_c.sum() * self.fraction))\n",
        "                mask_topk_c = torch.zeros_like(max_prob_c.flatten(), dtype=torch.bool)\n",
        "                mask_topk_c[idx_c] = 1\n",
        "                mask_c &= mask_topk_c.unflatten(dim=0, sizes=max_prob_c.size())\n",
        "                mask_topk |= mask_c\n",
        "        return mask_prob | mask_topk\n",
        "\n",
        "    def get_batch_mask(self, pred, pseudo_lab):\n",
        "        b, _, _, _ = pred.size()\n",
        "        mask = torch.stack([self.get_image_mask(pb, pl) for pb, pl in zip(F.softmax(pred, dim=1), pseudo_lab)], dim=0)\n",
        "        return mask\n",
        "\n",
        "    def get_pseudo_lab(self, pred, imgs=None, return_mask_fract=False, model=None):\n",
        "        teacher = self.teacher if model is None else model\n",
        "        if teacher is not None:\n",
        "            with torch.no_grad():\n",
        "                try:\n",
        "                    pred = teacher(imgs)['out']\n",
        "                except:\n",
        "                    pred = teacher(imgs)\n",
        "                pseudo_lab = pred.detach().max(1)[1]\n",
        "        else:\n",
        "            pseudo_lab = pred.detach().max(1)[1]\n",
        "        mask = self.get_batch_mask(pred, pseudo_lab)\n",
        "        pseudo_lab[~mask] = self.ignore_index\n",
        "        if return_mask_fract:\n",
        "            return pseudo_lab, F.softmax(pred, dim=1), mask.sum() / mask.numel()\n",
        "        return pseudo_lab\n",
        "\n",
        "    def forward(self, pred, imgs=None):\n",
        "        pseudo_lab = self.get_pseudo_lab(pred, imgs)\n",
        "        loss = F.cross_entropy(input=pred, target=pseudo_lab, ignore_index=self.ignore_index, reduction='none')\n",
        "        return loss.mean() * self.lambda_selftrain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgOyuX-JmUmW"
      },
      "source": [
        "### Client class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "q9bqWSV_mLYn"
      },
      "outputs": [],
      "source": [
        "class Client():\n",
        "  #def __init__(self, client_id, dataset, model, logger, writer, args, batch_size, world_size, rank, device=None, **kwargs):\n",
        "  def __init__(self, client_id, dataset, model,  pseudo_lab=False, teacher_model=None):\n",
        "    self.id = client_id\n",
        "    self.dataset = dataset\n",
        "    self.model = model #copy.deepcopy(model)\n",
        "    self.device = DEVICE\n",
        "    self.batch_size = BATCH_SIZE\n",
        "    self.loader = DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
        "\n",
        "    # ADDED\n",
        "    self.pseudo_lab = pseudo_lab\n",
        "    self.teacher_model = copy.deepcopy(teacher_model)\n",
        "    # Define loss function\n",
        "    if self.pseudo_lab:\n",
        "      self.criterion = SelfTrainingLoss()\n",
        "      self.criterion.set_teacher(self.teacher_model)\n",
        "    else:\n",
        "      self.criterion = nn.CrossEntropyLoss(ignore_index= 255)\n",
        "\n",
        "  def client_train(self):\n",
        "    \n",
        "    num_train_samples = len(self.dataset)\n",
        "\n",
        "    parameters_to_optimize = self.model.parameters() \n",
        "    optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    self.model = self.model.to(DEVICE)\n",
        "    self.model.train() # Sets module in training mode\n",
        "\n",
        "    cudnn.benchmark # Calling this optimizes runtime\n",
        "    \n",
        "    # Start iterating over the epochs\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "      if epoch % LOG_FREQUENCY_EPOCH == 0: \n",
        "        print('Starting epoch {}/{}'.format(epoch+1, NUM_EPOCHS))\n",
        "\n",
        "      # Iterate over the dataset\n",
        "      for current_step, (images, labels) in enumerate(self.loader):\n",
        "        images = images.to(DEVICE, dtype=torch.float32)\n",
        "        labels = labels.to(DEVICE, dtype=torch.long)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predictions = self.model(images)\n",
        "        # ADDED\n",
        "        if self.pseudo_lab:\n",
        "            loss = self.criterion(predictions, images.squeeze())\n",
        "        else:\n",
        "            loss = self.criterion(predictions, labels.squeeze())\n",
        "\n",
        "        # Log loss\n",
        "        if current_step % LOG_FREQUENCY == 0:\n",
        "          print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "          wandb.log({f\"client/loss\":loss})\n",
        "\n",
        "        loss.backward()  # backward pass: computes gradients\n",
        "        optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "    return num_train_samples, copy.deepcopy(self.model.state_dict()) #generate_update\n",
        "\n",
        "  def test(self, metrics, ret_samples_ids=None, silobn_type=None, train_cl_bn_stats=None, loader=None):\n",
        "    return\n",
        "\n",
        "  def save_model(self, epochs, path, optimizer, scheduler):\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M7r_ywbmZrf"
      },
      "source": [
        "### Server class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "LZwDioj3mTrN"
      },
      "outputs": [],
      "source": [
        "class Server():\n",
        "  #def __init__(self, model, logger, writer, local_rank, lr, momentum, optimizer=None):\n",
        "  def __init__(self, model, lr= None , momentum= None, pseudo_lab=False):\n",
        "    self.model = copy.deepcopy(model)\n",
        "    self.model_params_dict = copy.deepcopy(self.model.state_dict())\n",
        "    self.selected_clients = []\n",
        "    self.updates = []\n",
        "    self.lr = lr\n",
        "    self.momentum = momentum\n",
        "    self.optimizer = optim.SGD(params=self.model.parameters(), lr=1, momentum=0.9)\n",
        "    self.total_grad = 0\n",
        "\n",
        "    # ADDED\n",
        "    self.pseudo_lab = pseudo_lab\n",
        "\n",
        "  def select_clients(self, my_round, possible_clients, num_clients=4):\n",
        "    num_clients = min(num_clients, len(possible_clients))\n",
        "    np.random.seed(my_round)\n",
        "    self.selected_clients = np.random.choice(possible_clients, num_clients, replace=False)\n",
        "\n",
        "  def set_clients_teacher(self, train_clients):\n",
        "    for c in train_clients:\n",
        "      c.criterion.set_teacher(copy.deepcopy(self.model))\n",
        "\n",
        "  \n",
        "  def _compute_client_delta(self, cmodel):\n",
        "      delta = OrderedDict.fromkeys(cmodel.keys())\n",
        "      for k, x, y in zip(self.model_params_dict.keys(), self.model_params_dict.values(), cmodel.values()):\n",
        "        delta[k] = y - x if \"running\" not in k and \"num_batches_tracked\" not in k else y\n",
        "      return delta\n",
        "  \n",
        "  def train_round(self):\n",
        "\n",
        "    self.optimizer.zero_grad()\n",
        "\n",
        "    clients = self.selected_clients\n",
        "\n",
        "    for i, c in enumerate(clients):\n",
        "        print(f\"CLIENT {i + 1}/{len(clients)} -> {c.id}:\")\n",
        "\n",
        "        c.model.load_state_dict(self.model_params_dict) # load_server_model_on_client\n",
        "        out = c.client_train()\n",
        "\n",
        "        num_samples, update = out\n",
        "\n",
        "        update = self._compute_client_delta(update)\n",
        "        \n",
        "        self.updates.append((num_samples, update))\n",
        "    return \n",
        "\n",
        "  def _server_opt(self, pseudo_gradient):\n",
        "    for n, p in self.model.named_parameters():\n",
        "        p.grad = -1.0 * pseudo_gradient[n]\n",
        "\n",
        "    self.optimizer.step()\n",
        "\n",
        "    bn_layers = OrderedDict(\n",
        "        {k: v for k, v in pseudo_gradient.items() if \"running\" in k or \"num_batches_tracked\" in k})\n",
        "    self.model.load_state_dict(bn_layers, strict=False)\n",
        "\n",
        "  def _aggregation(self):\n",
        "    total_weight = 0.\n",
        "    base = OrderedDict()\n",
        "\n",
        "    for (client_samples, client_model) in self.updates:\n",
        "\n",
        "        total_weight += client_samples\n",
        "        for key, value in client_model.items():\n",
        "            if key in base:\n",
        "                base[key] += client_samples * value.type(torch.FloatTensor)\n",
        "            else:\n",
        "                base[key] = client_samples * value.type(torch.FloatTensor)\n",
        "    averaged_sol_n = copy.deepcopy(self.model_params_dict)\n",
        "    for key, value in base.items():\n",
        "        if total_weight != 0:\n",
        "            averaged_sol_n[key] = value.to('cuda') / total_weight\n",
        "\n",
        "    return averaged_sol_n\n",
        "\n",
        "  def _get_model_total_grad(self):\n",
        "    total_norm = 0\n",
        "    for name, p in self.model.named_parameters():\n",
        "        if p.requires_grad:\n",
        "            param_norm = p.grad.data.norm(2)\n",
        "            total_norm += param_norm.item() ** 2\n",
        "    total_grad = total_norm ** 0.5\n",
        "    return total_grad\n",
        "\n",
        "  def update_model(self):\n",
        "    \"\"\"FedAvg on the clients' updates for the current round.\n",
        "    Weighted average of self.updates, where the weight is given by the number\n",
        "    of samples seen by the corresponding client at training time.\n",
        "    Saves the new central model in self.client_model and its state dictionary in self.model\n",
        "    \"\"\"\n",
        "\n",
        "    averaged_sol_n = self._aggregation()\n",
        "    \n",
        "    self._server_opt(averaged_sol_n)\n",
        "    self.total_grad = self._get_model_total_grad()\n",
        "    self.model_params_dict = copy.deepcopy(self.model.state_dict())\n",
        "\n",
        "    self.updates = []\n",
        "\n",
        "  def test_model(self, clients_to_test, metrics, ret_samples_bool=False, silobn_type=''):\n",
        "    return "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lbhJabt6WLA"
      },
      "source": [
        "### Creating MIOU dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ZpLte4Yj6WLB"
      },
      "outputs": [],
      "source": [
        "if SPLIT == 1:\n",
        "  filename = f'uniform{PARTITION}.json'\n",
        "else:\n",
        "  filename = f'heterogeneous{PARTITION}.json'\n",
        "\n",
        "miou_dataset = Cityscapes(root=CTSC_ROOT, transform=transforms, cl19 = cl19, filename=filename, id_client = random.randint(0,TOT_CLIENTS-1))\n",
        "miou_dataloader = DataLoader(miou_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBAanwmdyp1a"
      },
      "source": [
        "### Setup clients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ccKOv4I8mAZU"
      },
      "outputs": [],
      "source": [
        "def setup_clients(n_client, model):\n",
        "\n",
        "  clients = []\n",
        "  if SPLIT == 1:\n",
        "    filename=f\"uniform{PARTITION}.json\"\n",
        "  else:\n",
        "    filename=f'heterogeneuos{PARTITION}.json'\n",
        "    \n",
        "  for i in range(n_client):\n",
        "    train_dataset = Cityscapes(root=CTSC_ROOT, transform=transforms, cl19 = cl19,filename=filename,  id_client = i)\n",
        "    client = Client(client_id = i, dataset = train_dataset, model = model, pseudo_lab = PSEUDO_LAB, teacher_model=model)\n",
        "    clients.append(client)\n",
        "    \n",
        "  return clients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKXRDgW3ytIu"
      },
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5lRok1qlMDi",
        "outputId": "6e41ac59-4ed8-4117-ad39-23ce01c88556"
      },
      "outputs": [],
      "source": [
        "model = BiSeNetV2(NUM_CLASSES, output_aux=False, pretrained=True)\n",
        "# Load the model from the previous step\n",
        "if ckpt_path in os.listdir(CKPT_DIR):\n",
        "  checkpoint = torch.load(os.path.join(CKPT_DIR,ckpt_path))\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "else:\n",
        "  raise FileNotFoundError\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "wandb.watch(model, log='all')\n",
        "\n",
        "model_path = \"./models/Step5/\"\n",
        "if not os.path.exists(model_path):\n",
        "  print(\"creating models directory\")\n",
        "  os.makedirs(model_path)\n",
        "\n",
        "\n",
        "train_clients = setup_clients(n_client = TOT_CLIENTS, model = model)\n",
        "print(len(train_clients))\n",
        "\n",
        "server = Server(model, lr=LR, momentum = MOMENTUM, pseudo_lab=PSEUDO_LAB)\n",
        "\n",
        "for r in range(N_ROUND):\n",
        "  print(f\"ROUND {r + 1}/{N_ROUND}: Training {CLIENT_PER_ROUND} Clients...\")\n",
        "  if r % T_ROUND == 0: \n",
        "    print('Teacher model updated')\n",
        "    server.set_clients_teacher(train_clients)\n",
        "  server.select_clients(r, train_clients, num_clients=CLIENT_PER_ROUND) \n",
        "  server.train_round()\n",
        "  server.update_model()\n",
        "\n",
        "  # TODO: move validation inside Server class\n",
        "  miou = compute_miou(net=server.model, val_dataloader=miou_dataloader)\n",
        "  wandb.log({\"server/miou\": miou})\n",
        "  print(f\"Validation MIoU: {miou}\")\n",
        "  \n",
        "  if r%CHECKPOINTS == 0:\n",
        "    print(f\"Saving the model\")\n",
        "    torch.save(model.state_dict(), model_path+f\"model{'_FDA' if FDA else ''}_T{T_ROUND}_P{PARTITION}_S{SPLIT}_round{r:02}.pth\")\n",
        "\n",
        "torch.save(model.state_dict(), model_path+f\"model{'_FDA' if FDA else ''}_T{T_ROUND}_P{PARTITION}_S{SPLIT}.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZayEdybd5d00"
      },
      "source": [
        "### Creating Validation Dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "85nv4t4i5N94"
      },
      "outputs": [],
      "source": [
        "val_dataset = Cityscapes(root=CTSC_ROOT, transform=transforms, cl19 = cl19, filename=f\"test_{PARTITION}.txt\")\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extracting the best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "#ckpt = torch.load(torch.load_state_dict(model_path+f\"model{'_FDA' if FDA else ''}_T{T_ROUND}_P{PARTITION}_S{SPLIT}.pth\"))\n",
        "model = BiSeNetV2(NUM_CLASSES, output_aux=False, pretrained=True)\n",
        "model.load_state_dict(torch.load(model_path+f\"model{'_FDA' if FDA else ''}_T{T_ROUND}_P{PARTITION}_S{SPLIT}.pth\"))\n",
        "model.eval()\n",
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I35WpYCP5khb"
      },
      "source": [
        "### Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "yHyGB6Is5k0_",
        "outputId": "244d469d-c03e-446d-f42e-6e456197ca7e"
      },
      "outputs": [],
      "source": [
        "print(\"computing miou ...\")\n",
        "miou = compute_miou(net=model, val_dataloader=val_dataloader)\n",
        "print(\"Validation MIoU: {}\".format(miou))\n",
        "wandb.log({\"val/miou\": miou})\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcA8Mdfm5lO3"
      },
      "source": [
        "### Validation Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9WdFuFD55loX",
        "outputId": "8b15a2bd-18f3-4f6f-da0e-ce7e79755ac7"
      },
      "outputs": [],
      "source": [
        "print(\"validation plot : \")\n",
        "validation_plot(net=model, val_dataloader=val_dataloader, n_images=5)\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 6 : MOBILENET V2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup for WanDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.login()\n",
        "\n",
        "transformer_dictionary = {\n",
        "    \"random-horizontal-flip\":RANDOM_HORIZONTAL_FLIP,\n",
        "    \"color-jitter\":COLOR_JITTER,\n",
        "    \"random-rotation\":RANDOM_ROTATION,\n",
        "    \"random-crop\":RANDOM_CROP,\n",
        "    \"random-vertical-flip\":RANDOM_VERTICAL_FLIP,\n",
        "    \"central-crop\":CENTRAL_CROP,\n",
        "    \"random-resized-crop\":RANDOM_RESIZE_CROP,\n",
        "    \"resize\":RESIZE,\n",
        "    }\n",
        "\n",
        "config = {\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"lr\": LR,\n",
        "        \"num_epochs\": NUM_EPOCHS,\n",
        "        #\"step_size\": STEP_SIZE,\n",
        "        \"transformers\": transformer_dictionary,\n",
        "    }\n",
        "name = f\"Step_6_{PARTITION}_lr{LR}_bs{BATCH_SIZE}_e{NUM_EPOCHS}\"\n",
        "wandb.init(\n",
        "    project = \" STEP6\",\n",
        "    # entity = \"lor-bellino\",\n",
        "    config = config,\n",
        "    name = name,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MobileNet V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "\n",
        "# from .mobilenetv2 import MobileNetV2\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "from torchvision.models.segmentation.deeplabv3 import DeepLabV3, DeepLabHead\n",
        "from torchvision._internally_replaced_utils import load_state_dict_from_url\n",
        "\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "__all__ = ['mobilenetv2', 'MobileNetV2']\n",
        "\n",
        "\n",
        "def _make_divisible(v, divisor, min_value=None):\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "\n",
        "def conv_3x3_bn(inp, oup, stride):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.ReLU6(inplace=True)\n",
        "    )\n",
        "\n",
        "\n",
        "def conv_1x1_bn(inp, oup):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.ReLU6(inplace=True)\n",
        "    )\n",
        "\n",
        "\n",
        "class InvertedResidual(nn.Module):\n",
        "    def __init__(self, inp, oup, stride, expand_ratio):\n",
        "        super(InvertedResidual, self).__init__()\n",
        "        assert stride in [1, 2]\n",
        "\n",
        "        hidden_dim = round(inp * expand_ratio)\n",
        "        self.identity = stride == 1 and inp == oup\n",
        "\n",
        "        if expand_ratio == 1:\n",
        "            self.conv = nn.Sequential(\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.ReLU6(inplace=True),\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "        else:\n",
        "            self.conv = nn.Sequential(\n",
        "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.ReLU6(inplace=True),\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.ReLU6(inplace=True),\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.identity:\n",
        "            return x + self.conv(x)\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "class MobileNetV2(nn.Module):\n",
        "    def __init__(self, num_classes=1000, width_mult=1., in_channels=3):\n",
        "        super(MobileNetV2, self).__init__()\n",
        "        self.cfgs = [\n",
        "            [1, 16, 1, 1],\n",
        "            [6, 24, 2, 2],\n",
        "            [6, 32, 3, 2],\n",
        "            [6, 64, 4, 2],\n",
        "            [6, 96, 3, 1],\n",
        "            [6, 160, 3, 2],\n",
        "            [6, 320, 1, 1],\n",
        "        ]\n",
        "\n",
        "        input_channel = _make_divisible(32 * width_mult, 4 if width_mult == 0.1 else 8)\n",
        "        layers = [conv_3x3_bn(in_channels, input_channel, 2)]\n",
        "        block = InvertedResidual\n",
        "        for t, c, n, s in self.cfgs:\n",
        "            output_channel = _make_divisible(c * width_mult, 4 if width_mult == 0.1 else 8)\n",
        "            for i in range(n):\n",
        "                layers.append(block(input_channel, output_channel, s if i == 0 else 1, t))\n",
        "                input_channel = output_channel\n",
        "\n",
        "        output_channel = _make_divisible(1280 * width_mult, 4 if width_mult == 0.1 else 8) if width_mult > 1.0 else 1280\n",
        "        layers.append(conv_1x1_bn(input_channel, output_channel))\n",
        "        self.features = nn.Sequential(*layers)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.classifier = nn.Linear(output_channel, num_classes)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "\n",
        "def mobilenetv2(**kwargs):\n",
        "    return MobileNetV2(**kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def _deeplabv3_mobilenetv2(\n",
        "        backbone: MobileNetV2,\n",
        "        num_classes: int,\n",
        ") -> DeepLabV3:\n",
        "    backbone = backbone.features\n",
        "\n",
        "    out_pos = len(backbone) - 1\n",
        "    out_inplanes = backbone[out_pos][0].out_channels\n",
        "    return_layers = {str(out_pos): \"out\"}\n",
        "\n",
        "    backbone = create_feature_extractor(backbone, return_layers)\n",
        "    classifier = DeepLabHead(out_inplanes, num_classes)\n",
        "\n",
        "    return DeepLabV3(backbone, classifier)\n",
        "\n",
        "\n",
        "def deeplabv3_mobilenetv2(\n",
        "        num_classes: int = 21,\n",
        "        in_channels: int = 3\n",
        ") -> DeepLabV3:\n",
        "\n",
        "    width_mult = 1\n",
        "    backbone = MobileNetV2(width_mult=width_mult, in_channels=in_channels)\n",
        "    model_urls = {\n",
        "        0.5: 'https://github.com/d-li14/mobilenetv2.pytorch/raw/master/pretrained/mobilenetv2_0.5-eaa6f9ad.pth',\n",
        "        1.: 'https://github.com/d-li14/mobilenetv2.pytorch/raw/master/pretrained/mobilenetv2_1.0-0c6065bc.pth'}\n",
        "    state_dict = load_state_dict_from_url(model_urls[width_mult], progress=True)\n",
        "    state_dict_updated = state_dict.copy()\n",
        "    for k, v in state_dict.items():\n",
        "        if 'features' not in k and 'classifier' not in k:\n",
        "            state_dict_updated[k.replace('conv', 'features.18')] = v\n",
        "            del state_dict_updated[k]\n",
        "\n",
        "    if in_channels == 4:\n",
        "        aux = torch.zeros((32, 4, 3, 3))\n",
        "        aux[:, 0, :, :] = copy.deepcopy(state_dict_updated['features.0.0.weight'][:, 0, :, :])\n",
        "        aux[:, 1, :, :] = copy.deepcopy(state_dict_updated['features.0.0.weight'][:, 1, :, :])\n",
        "        aux[:, 2, :, :] = copy.deepcopy(state_dict_updated['features.0.0.weight'][:, 2, :, :])\n",
        "        aux[:, 3, :, :] = copy.deepcopy(state_dict_updated['features.0.0.weight'][:, 2, :, :])\n",
        "        state_dict_updated['features.0.0.weight'] = aux\n",
        "    backbone.load_state_dict(state_dict_updated, strict=False)\n",
        "\n",
        "    model = _deeplabv3_mobilenetv2(backbone, num_classes)\n",
        "    model.task = 'segmentation'\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transforms = setup_transform()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "if PARTITION == \"A\":\n",
        "    train_dataset = Cityscapes(\n",
        "        root=ROOT_DIR,\n",
        "        transform=transforms,\n",
        "        cl19=True,\n",
        "        filename=\"train_A.txt\",\n",
        "    )\n",
        "elif PARTITION == \"B\":\n",
        "    train_dataset = Cityscapes(\n",
        "        root=ROOT_DIR,\n",
        "        transform=transforms,\n",
        "        cl19=True,\n",
        "        filename=\"train_B.txt\",\n",
        "    )\n",
        "else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    drop_last=True,\n",
        ")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if LOAD_CKPT:\n",
        "    print(\"loading the model\")\n",
        "    model = deeplabv3_mobilenetv2(num_classes=NUM_CLASSES)\n",
        "    # model = BiSeNetV2(NUM_CLASSES, output_aux=False, pretrained=False)\n",
        "    model.load_state_dict(torch.load(\"models/\" + LOAD_CKPT_PATH))\n",
        "else:\n",
        "    print(\"creating a new model\")\n",
        "    model = deeplabv3_mobilenetv2(num_classes=NUM_CLASSES)\n",
        "    # model = BiSeNetV2(NUM_CLASSES, output_aux=False, pretrained=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Miou dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if SPLIT == 1:\n",
        "  filename = f'uniform{PARTITION}.json'\n",
        "else:\n",
        "  filename = f'heterogeneous{PARTITION}.json'\n",
        "\n",
        "miou_dataset = Cityscapes(root=CTSC_ROOT, transform=transforms, cl19 = cl19, filename=filename, id_client = random.randint(0,TOT_CLIENTS-1))\n",
        "miou_dataloader = DataLoader(miou_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "parameters_to_optimize = model.parameters()\n",
        "optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "cudnn.benchmark  # Calling this optimizes runtime\n",
        "\n",
        "epochs = []\n",
        "\n",
        "wandb.watch(model, log=\"all\")\n",
        "\n",
        "current_step = 0\n",
        "\n",
        "best_miou = 0\n",
        "\n",
        "# Start iterating over the epochs\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"Starting epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
        "    epochs.append(epoch + 1)\n",
        "    for images, labels in train_dataloader:\n",
        "       images = images.to(DEVICE, dtype=torch.float32)\n",
        "       labels = labels.to(DEVICE, dtype=torch.long)\n",
        "       model.train()\n",
        "       optimizer.zero_grad()\n",
        "\n",
        "       predictions = model(images)\n",
        "       loss = criterion(predictions, labels.squeeze())\n",
        "\n",
        "       # Log loss\n",
        "       if current_step % LOG_FREQUENCY == 0:\n",
        "           print(f\"Step {current_step}, Loss {loss.item()}\")\n",
        "           wandb.log({\"train/loss\": loss})\n",
        "        # Compute gradients for each layer and update weights\n",
        "       loss.backward()\n",
        "       optimizer.step()\n",
        "\n",
        "       current_step += 1\n",
        "    miou = compute_miou(model, miou_dataloader)\n",
        "    wandb.log({\"train/miou\": miou})\n",
        "    if miou > best_miou:\n",
        "        print(f\"Saving the model with miou {miou:.2} (best so far)\")\n",
        "        best_miou = miou\n",
        "        torch.save(\n",
        "               {\n",
        "                    \"epoch\": epoch + 1,\n",
        "                    \"model_state_dict\": model.state_dict(),\n",
        "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                    \"loss\": loss,\n",
        "                    \"miou\": miou,\n",
        "                },\n",
        "                CKPT_PATH + f\"step4_{PARTITION}_S{SPLIT}_{'FDA' if FDA else 'pretraining'}_{miou:.2}.pth\",\n",
        "            )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Validation Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_dataset = Cityscapes(root=CTSC_ROOT, transform=transforms, cl19 = cl19, filename=f\"test_{PARTITION}.txt\")\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extract the best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "ckpt = torch.load(CKPT_PATH + f\"step4_{PARTITION}_S{SPLIT}_{'FDA' if FDA else 'pretraining'}_{best_miou:.2}.pth\")\n",
        "model = BiSeNetV2(NUM_CLASSES, output_aux=False, pretrained=True)\n",
        "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
        "model.eval()\n",
        "model.to(DEVICE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"computing miou ...\")\n",
        "miou = compute_miou(net=model, val_dataloader=val_dataloader)\n",
        "print(\"Validation MIoU: {}\".format(miou))\n",
        "wandb.log({\"val/miou\": miou})\n",
        "wandb.finish()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validation Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"validation plot : \")\n",
        "validation_plot(net=model, val_dataloader=val_dataloader, n_images=5)\n",
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "SnQPPHnH6-9f",
        "7_32w_Ap6-9g",
        "rVGBAN-H6-9g",
        "u84oVILW6-9g",
        "wf3LLgTc6-9h",
        "TsPNj7yC6-9i",
        "zVFRMBjZ1t-S",
        "F1SkAQlk6-9i",
        "rOrGP70fvC9r",
        "JKcn7wbRvFo2",
        "G9nJvrc96-9j",
        "yP86iZXo6-9j",
        "a4SzVD3y6-9k",
        "T-V9jn1M6-9m",
        "ICF_u3x46-9n",
        "zCZd4PE9pHKm",
        "x4bl6HbppHKn"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e6e2a00f5b54c6b946780c026594b1f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "125a6602f58447478603015a1d7e0895": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32ee5d0ce27d429cb2aa51812267d836": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64577185fcf3462986b957147ea263a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8dec5d9759984988ac05d56aadc8349a",
            "placeholder": "​",
            "style": "IPY_MODEL_a8d0e49771b4498f9b1e7b6f8a6a4c8a",
            "value": "0.002 MB of 0.011 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "68ea1b36b1be4291a81fdd99cdc1d77b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_125a6602f58447478603015a1d7e0895",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_86298e6fbe7446c8b026105e87e467c9",
            "value": 0.1747826820828762
          }
        },
        "6b5fce34081348e6bf995d1b2640708d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32ee5d0ce27d429cb2aa51812267d836",
            "placeholder": "​",
            "style": "IPY_MODEL_9e6f29242e9b47b7bddd20ae5677abf8",
            "value": "0.001 MB of 0.619 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "741f843e369442cea3acae6a256e630c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64577185fcf3462986b957147ea263a5",
              "IPY_MODEL_68ea1b36b1be4291a81fdd99cdc1d77b"
            ],
            "layout": "IPY_MODEL_0e6e2a00f5b54c6b946780c026594b1f"
          }
        },
        "85c1ed2d3af64782813a5dacb09d7f1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f82a991db17e485cad5e8a11c6641ea2",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_91025867a63f4ef9b5cc1fd54cc6c9ca",
            "value": 0.0018587818967589996
          }
        },
        "86298e6fbe7446c8b026105e87e467c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8dec5d9759984988ac05d56aadc8349a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91025867a63f4ef9b5cc1fd54cc6c9ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "98b86aed00b94600b928ed49224501da": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e6f29242e9b47b7bddd20ae5677abf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8d0e49771b4498f9b1e7b6f8a6a4c8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7ccda91edf7486f9bad5100ec2ede38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6b5fce34081348e6bf995d1b2640708d",
              "IPY_MODEL_85c1ed2d3af64782813a5dacb09d7f1d"
            ],
            "layout": "IPY_MODEL_98b86aed00b94600b928ed49224501da"
          }
        },
        "f82a991db17e485cad5e8a11c6641ea2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
